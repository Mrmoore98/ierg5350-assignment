{"cells":[{"cell_type":"markdown","metadata":{},"source":[" # IERG 5350 Assignment 1: Tabular Reinforcement Learning\n","\n"," *2020-2021 Term 1, IERG 5350: Reinforcement Learning. Department of Information Engineering, The Chinese University of Hong Kong. Course Instructor: Professor ZHOU Bolei. Assignment author: PENG Zhenghao, SUN Hao, ZHAN Xiaohang.*\n",""]},{"cell_type":"markdown","metadata":{},"source":[" | Student Name | Student ID |\n"," | :----: | :----: |\n"," | LIU Yicheng | 1155152886 |\n","\n"," ------"]},{"cell_type":"markdown","metadata":{},"source":[" Welcome to the assignment 1 of our RL course. The objective of this assignment is for you to understand the classic methods used in tabular reinforcement learning.\n","\n"," This assignment has the following sections:\n","\n","  - Section 1: Warm-up on the RL environment (35 points)\n","  - Section 2: Implementation of model-based family of algorithms: policy iteration and value iteration. (65 points)\n","\n"," You need to go through this self-contained notebook, which contains **21 TODOs** in part of the cells and has special `[TODO]` signs. You need to finish all TODOs. Some of them may be easy such as uncommenting a line, some of them may be difficult such as implementing a function. You can find them by searching the `[TODO]` symbol. However, we suggest you to go through the documents step by step, which will give you a better sense of the content.\n","\n"," You are encouraged to add more code on extra cells at the end of the each section to investigate the problems you think interesting. At the end of the file, we left a place for you to optionaly write comments (Yes, please give us some either negative or positive rewards so we can keep improving the assignment!).\n","\n"," Please report any code bugs to us via **github issues**.\n","\n"," Before you get start, remember to follow the instruction at https://github.com/cuhkrlcourse/ierg5350-assignment to setup your environment."]},{"cell_type":"markdown","metadata":{},"source":[" Now start running the cells sequentially (by `ctrl + enter` or `shift + enter`) to avoid unnecessary errors by skipping some cells.\n","\n","\n"," ## Section 1: Warm-up on the RL environment\n","\n"," (35/100 points)\n","\n"," In this section, we will go through the basic concepts of RL environments using OpenAI Gym. Besides, you will get the first sense of the toy environment we will use in the rest of the assignment.\n","\n"," Every Gym environment should contain the following attributes:\n","\n"," 1. `env.step(action)` To step the environment by applying `action`. Will return four things: `observation, reward, done, info`, wherein `done` is a boolean value indicating whether this **episode** is finished. `info` may contain some information the user is interested in, we do not use it.\n"," 2. `env.reset()` To reset the environment, back to the initial state. Will return the initial observation.\n"," 3. `env.render()` To render the current state of the environment for human-being\n"," 4. `env.action_space` The allowed action format. In our case, it is `Discrete(4)` which means the action is an integer in the range [0, 1, 2, 3]. Therefore the `action` for `step(action)` should obey the limit of the action space.\n"," 5. `env.observation_space` The observation space.\n"," 6. `env.seed(seed)` To set the random seed of the environment. So the result is replicable.\n","\n"," Note that the word **episode** means the process that an agent interacts with the environment from the initial state to the terminal state. Within one episode, the agent will only receive one `done=True`, when it goes to the terminal state (the agent is dead or the game is over).\n","\n"," We will use \"FrozenLake8x8-v0\" as our environment. In this environment, the agent controls the movement of a character in a grid world. Some tiles of the grid are walkable, and others lead to the agent falling into the water. Additionally, the movement direction of the agent is uncertain and only partially depends on the chosen direction. The agent is rewarded for finding a walkable path to a goal tile. The meaning of each character:\n","\n"," 1. S : starting point, safe\n"," 2. F : frozen surface, safe\n"," 3. H : hole, fall to your doom\n"," 4. G : goal, where the frisbee is located\n",""]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["# Run this cell without modification\n","\n","# Import some packages that we need to use\n","from utils import *\n","import gym\n","import numpy as np\n","from collections import deque\n",""]},{"cell_type":"markdown","metadata":{},"source":[" ### Section 1.1: Make the environment\n","\n"," You need to know\n","\n"," 1. How to make an environment\n"," 2. How to set the random seed of environment\n"," 3. What is observation space and action space"]},{"cell_type":"code","execution_count":2,"metadata":{"tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":"Current observation space: Discrete(64)\nCurrent action space: Discrete(4)\n0 in action space? True\n5 in action space? False\n"}],"source":["# Solve the TODOs and remove `pass`\n","\n","# [TODO] Just a reminder. Do you add your name and student \n","# ID in the table at top of the notebook?\n","# yes!\n","\n","# Create the environment\n","env = gym.make('FrozenLake8x8-v0')\n","\n","# You need to reset the environment immediately after instantiating env. \n","env.reset()  # [TODO] uncomment this line\n","\n","# Seed the environment\n","env.seed(0)  # [TODO] uncomment this line\n","\n","print(\"Current observation space: {}\".format(env.observation_space))\n","print(\"Current action space: {}\".format(env.action_space))\n","print(\"0 in action space? {}\".format(env.action_space.contains(0)))\n","print(\"5 in action space? {}\".format(env.action_space.contains(5)))\n",""]},{"cell_type":"markdown","metadata":{},"source":[" ### Section 1.2: Play the environment with random actions\n","\n"," You need to know\n","\n"," 1. How to step the environment\n"," 2. How to render the environment"]},{"cell_type":"code","execution_count":3,"metadata":{"tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":"(Right)\nSFFFFFFF\nFFFFFFFF\nFFF\u001b[41mH\u001b[0mFFFF\nFFFFFHFF\nFFFHFFFF\nFHHFFFHF\nFHFFHFHF\nFFFHFFFG\nCurrent observation: 19\nCurrent reward: 0.0\nWhether we are done: True\ninfo: {'prob': 0.3333333333333333}\n"}],"source":["# Solve the TODOs and remove `pass`\n","\n","# Run 1000 steps for test, terminate if done.\n","# You can run this cell multiples times.\n","env.reset()\n","\n","while True:\n","    # take random action\n","    # [TODO] Uncomment next line\n","    obs, reward, done, info = env.step(env.action_space.sample())\n","\n","    # render the environment\n","    env.render()  # [TODO] Uncomment this line\n","\n","    print(\"Current observation: {}\\nCurrent reward: {}\\n\"\n","          \"Whether we are done: {}\\ninfo: {}\".format(\n","        obs, reward, done, info\n","    ))\n","    wait(sleep=0.5)\n","\n","    # [TODO] terminate the loop if done\n","    if done:\n","       break\n",""]},{"cell_type":"markdown","metadata":{},"source":[" ### Section 1.3: Define the evaluation function to value the random baseline\n","\n"," Now we need to define an evaluation function to evaluate a given policy (a function where the input is observation and the output is action). This is convenient for future evaluation.\n","\n"," As a reminder, you should create a `FrozenLake8x8-v0` environment instance by default, reset it after each episode (and at the beginning), step the environment, and terminate episode if done.\n","\n"," After implementing the `evaluate` function, run the next cell to check whether you are right."]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["# Solve the TODOs and remove `pass`\n","\n","def _render_helper(env):\n","    env.render()\n","    wait(sleep=0.2)\n","\n","\n","def evaluate(policy, num_episodes, seed=0, env_name='FrozenLake8x8-v0', render=False):\n","    \"\"\"[TODO] You need to implement this function by yourself. It\n","    evaluate the given policy and return the mean episode reward.\n","    We use `seed` argument for testing purpose.\n","    You should pass the tests in the next cell.\n","\n","    :param policy: a function whose input is an interger (observation)\n","    :param num_episodes: number of episodes you wish to run\n","    :param seed: an interger, used for testing.\n","    :param env_name: the name of the environment\n","    :param render: a boolean flag. If true, please call _render_helper\n","    function.\n","    :return: the averaged episode reward of the given policy.\n","    \"\"\"\n","\n","    # Create environment (according to env_name, we will use env other than 'FrozenLake8x8-v0')\n","    env = gym.make(env_name)\n","\n","    # Seed the environment\n","    env.seed(seed)\n","\n","    # Build inner loop to run.\n","    # For each episode, do not set the limit.\n","    # Only terminate episode (reset environment) when done = True.\n","    # The episode reward is the sum of all rewards happen within one episode.\n","    # Call the helper function `render(env)` to render\n","    rewards = []\n","    for i in range(num_episodes):\n","        # reset the environment\n","        obs = env.reset()\n","        act = policy(obs)\n","        \n","        ep_reward = 0\n","        while True:\n","            # [TODO] run the environment and terminate it if done, collect the\n","            # reward at each step and sum them to the episode reward.\n","            obs, reward, done, info = env.step(act)\n","            act = policy(obs)\n","            ep_reward += reward\n","            if render:\n","                _render_helper(env)\n","            if done:\n","                break\n","        rewards.append(ep_reward)\n","\n","    return np.mean(rewards)\n","\n","# # [TODO] Run next cell to test your implementation!\n","\n",""]},{"cell_type":"code","execution_count":5,"metadata":{"tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":"Test Passed!\n\nAs a baseline, the mean episode reward of a hand-craft agent is:  0.065\n"}],"source":["# Run this cell without modification\n","\n","# Run this cell to test the correctness of your implementation of `evaluate`.\n","LEFT = 0\n","DOWN = 1\n","RIGHT = 2\n","UP = 3\n","\n","def expert(obs):\n","    \"\"\"Go down if agent at the right edge, otherwise go right.\"\"\"\n","    return DOWN if (obs + 1) % 8 == 0 else RIGHT\n","\n","def assert_equal(seed, value, env_name):\n","    ret = evaluate(expert, 1000, seed, env_name=env_name)\n","    assert ret == value,     \"When evaluate on seed {}, 1000 episodes, in {} environment, the \"     \"averaged reward should be {}. But you get {}.\"     \"\".format(seed, env_name, value, ret)\n","      \n","assert_equal(0, 0.065, 'FrozenLake8x8-v0')\n","assert_equal(1, 0.059, 'FrozenLake8x8-v0')\n","assert_equal(2, 0.055, 'FrozenLake8x8-v0')\n","\n","assert_equal(0, 0.026, 'FrozenLake-v0')\n","assert_equal(1, 0.034, 'FrozenLake-v0')\n","assert_equal(2, 0.028, 'FrozenLake-v0')\n","\n","print(\"Test Passed!\")\n","print(\"\\nAs a baseline, the mean episode reward of a hand-craft \"\n","      \"agent is: \", evaluate(expert, 1000))\n",""]},{"cell_type":"markdown","metadata":{},"source":[" Congraduation! You have finished section 1 (if and only if not error happen at the above codes).\n","\n"," If you want to do more investigation, feel free to open new cells via `Esc + B` after the next cells and write codes in it, so that you can reuse some result in this notebook. Remember to write sufficient comments and documents to let others know what you are doing."]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["# You can do more inverstigation here if you wish. Leave it blank if you don't.\n",""]},{"cell_type":"markdown","metadata":{},"source":[" ------\n","\n"," ## Section 2: Model-based Tabular RL\n","\n"," (65/100 points)\n","\n"," We have learned how to use the Gym environment to run an episode, as well as how to interact between the agent (policy) and environment via `env.step(action)` to collect observation, reward, done, and possible extra information.\n","\n"," Now we need to build the basic tabular RL algorithm to solve this environment. **Note that compared to the model-free methods in the Sec.3, the algorithms in this section needs to access the internal information of the environment, namely the transition dynamics**. In our case, given a state and an action, we need to know which state current environment would jump to, and the probability of this happens, and the reward if the transition happens. You will see that we provide you a helper function `trainer._get_transitions(state, action)` that takes state and action as input and return you a list of possible transitions.\n","\n"," You will use a class to represent a Trainer, which seems to be over-complex for tabular RL. But we will use the same framework in the future assignments, or even in your future research. So it would be helpful for you to get familiar with how to implement an RL algorithm in a class-orientetd programming style, as a first step toward the implementation of state of the art RL algorithm in the future."]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["# Run this cell without modification\n","\n","class TabularRLTrainerAbstract:\n","    \"\"\"This is the abstract class for tabular RL trainer. We will inherent the specify \n","    algorithm's trainer from this abstract class, so that we can reuse the codes like\n","    getting the dynamic of the environment (self._get_transitions()) or rendering the\n","    learned policy (self.render()).\"\"\"\n","    \n","    def __init__(self, env_name='FrozenLake8x8-v0', model_based=True):\n","        self.env_name = env_name\n","        self.env = gym.make(self.env_name)\n","        self.action_dim = self.env.action_space.n\n","        self.obs_dim = self.env.observation_space.n\n","        \n","        self.model_based = model_based\n","\n","    def _get_transitions(self, state, act):\n","        \"\"\"Query the environment to get the transition probability,\n","        reward, the next state, and done given a pair of state and action.\n","        We implement this function for you. But you need to know the \n","        return format of this function.\n","        \"\"\"\n","        self._check_env_name()\n","        assert self.model_based, \"You should not use _get_transitions in \"             \"model-free algorithm!\"\n","        \n","        # call the internal attribute of the environments.\n","        # `transitions` is a list contain all possible next states and the \n","        # probability, reward, and termination indicater corresponding to it\n","        transitions = self.env.env.P[state][act]\n","\n","        # Given a certain state and action pair, it is possible\n","        # to find there exist multiple transitions, since the \n","        # environment is not deterministic.\n","        # You need to know the return format of this function: a list of dicts\n","        ret = []\n","        for prob, next_state, reward, done in transitions:\n","            ret.append({\n","                \"prob\": prob,\n","                \"next_state\": next_state,\n","                \"reward\": reward,\n","                \"done\": done\n","            })\n","        return ret\n","    \n","    def _check_env_name(self):\n","        assert self.env_name.startswith('FrozenLake')\n","\n","    def print_table(self):\n","        \"\"\"print beautiful table, only work for FrozenLake8X8-v0 env. We \n","        write this function for you.\"\"\"\n","        self._check_env_name()\n","        print_table(self.table)\n","\n","    def train(self):\n","        \"\"\"Conduct one iteration of learning.\"\"\"\n","        raise NotImplementedError(\"You need to override the \"\n","                                  \"Trainer.train() function.\")\n","\n","    def evaluate(self):\n","        \"\"\"Use the function you write to evaluate current policy.\n","        Return the mean episode reward of 1000 episodes when seed=0.\"\"\"\n","        result = evaluate(self.policy, 1000, env_name=self.env_name)\n","        return result\n","\n","    def render(self):\n","        \"\"\"Reuse your evaluate function, render current policy \n","        for one episode when seed=0\"\"\"\n","        evaluate(self.policy, 1, render=True, env_name=self.env_name)\n",""]},{"cell_type":"markdown","metadata":{},"source":[" ### Section 2.1: Policy Iteration\n","\n"," Recall the idea of policy iteration:\n","\n"," 1. Update the state value function, given all possible transitions in the environment.\n"," 2. Find the best policy that makes the most out of the current state value function.\n"," 3. If the best policy is identical to the previous one then stop the training. Otherwise, go to step 1.\n","\n"," In step 1, the way to update the state value function is by\n","\n"," $$v_{k+1} = E_{s'}[r(s, a)+\\gamma v_{k}(s')]$$\n","\n"," wherein the $a$ is given by current policy, $s'$ is next state, $r$ is the reward, $v_{k}(s')$ is the next state value given by the old (not updated yet) value function. The expectation is computed among all possible transitions (given a state and action pair, it is possible to have many different next states, since the environment is not deterministic).\n","\n"," In step 2, the best policy is to take the action with maximum expected return given a state:\n","\n"," $$a = {argmax}_a E_{s'}[r(s, a) + \\gamma v_{k}(s')]$$\n","\n"," Policy iteration algorithm has an outer loop (update policy, step 1 to 3) and an inner loop (fit the value function, within step 1). In each outer loop, we call once `trainer.train()`, where we call `trainer.update_value_function()` once to update the value function (the state value table). After that we call `trainer.update_policy()` to update the current policy. `trainer` object has a `trainer.policy` attribute, which is a function that takes observation as input and returns an action.\n","\n"," You should implement the trainer following the framework we already wrote for you. Please carefully go through the codes and finish all `TODO` in it."]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["# Solve the TODOs and remove `pass`\n","\n","class PolicyItertaionTrainer(TabularRLTrainerAbstract):\n","    def __init__(self, gamma=1.0, eps=1e-10, env_name='FrozenLake8x8-v0'):\n","        super(PolicyItertaionTrainer, self).__init__(env_name)\n","\n","        # discount factor\n","        self.gamma = gamma\n","\n","        # value function convergence criterion\n","        self.eps = eps\n","\n","        # build the value table for each possible observation\n","        self.table = np.zeros((self.obs_dim,))\n","\n","        # [TODO] you need to implement a random policy at the beginning.\n","        # It is a function that take an integer (state or say observation)\n","        # as input and return an interger (action).\n","        # remember, you can use self.action_dim to get the dimension (range)\n","        # of the action, which is an integer in range\n","        # [0, ..., self.action_dim - 1]\n","        # hint: generating random action at each call of policy may lead to\n","        #  failure of convergence, try generate random actions at initializtion\n","        #  and fix it during the training.\n","         \n","        self.random_tabular = np.random.randint(self.action_dim, size=self.obs_dim)\n","        self.policy = lambda x: self.random_tabular[x]\n","        # test your random policy\n","        test_random_policy(self.policy, self.env)\n","\n","    def train(self):\n","        \"\"\"Conduct one iteration of learning.\"\"\"\n","        # [TODO] value function may be need to be reset to zeros.\n","        # if you think it should, than do it. If not, then move on.\n","        # hint: the value function is equivalent to self.table,\n","        #  a numpy array with length 64.\n","        self.update_value_function()\n","        self.update_policy()\n","\n","\n","\n","    def update_value_function(self):\n","        count = 0  # count the steps of value updates\n","        while True:\n","            old_table = self.table.copy()\n","\n","            for state in range(self.obs_dim):\n","                act = self.policy(state)\n","                transition_list = self._get_transitions(state, act)\n","                \n","                state_value = 0\n","                for transition in transition_list:\n","                    prob = transition['prob']\n","                    reward = transition['reward']\n","                    next_state = transition['next_state']\n","                    done = transition['done']\n","                    # [TODO] what is the right state value?\n","                    # hint: you should use reward, self.gamma, old_table, prob,\n","                    # and next_state to compute the state value\n","                    state_value += prob*(reward+self.gamma*old_table[next_state])\n","                \n","                # update the state value\n","                self.table[state] = state_value\n","                \n","\n","            # [TODO] Compare the old_table and current table to\n","            #  decide whether to break the value update process.\n","            # hint: you should use self.eps, old_table and self.table\n","            should_break =  np.sum(np.abs(self.table-old_table)) < self.eps\n","            if should_break:\n","                \n","                break\n","            count += 1\n","            if count % 200 == 0:\n","                # disable this part if you think debug message annoying.\n","                print(\"[DEBUG]\\tUpdated values for {} steps. \"\n","                      \"Difference between new and old table is: {}\".format(\n","                    count, np.sum(np.abs(old_table - self.table))\n","                ))\n","            if count > 4000:\n","                print(\"[HINT] Are you sure your codes is OK? It shouldn't be \"\n","                      \"so hard to update the value function. You already \"\n","                      \"use {} steps to update value function within \"\n","                      \"single iteration.\".format(count))\n","            if count > 6000:\n","                raise ValueError(\"Clearly your code has problem. Check it!\")\n","\n","    def update_policy(self):\n","        \"\"\"You need to define a new policy function, given current\n","        value function. The best action for a given state is the one that\n","        has greatest expected return.\n","\n","        To optimize computing efficiency, we introduce a policy table,\n","        which take state as index and return the action given a state.\n","        \"\"\"\n","        policy_table = np.zeros([self.obs_dim, ], dtype=np.int)\n","\n","        for state in range(self.obs_dim):\n","            state_action_values = [0] * self.action_dim\n","\n","            # [TODO] assign the action with greatest \"value\"\n","            # to policy_table[state]\n","            # hint: what is the proper \"value\" here?\n","            #  you should use table, gamma, reward, prob,\n","            #  next_state and self._get_transitions() function\n","            #  as what we done at self.update_value_function()\n","            #  Bellman equation may help.\n","            best_action = 0\n","            best_state_action_value = 0\n","            for act in range(self.action_dim):\n","                transition_list = self._get_transitions(state, act)\n","                state_action_value =0\n","                for transition in transition_list:\n","                    prob = transition['prob']\n","                    reward = transition['reward']\n","                    next_state = transition['next_state']\n","                    done = transition['done']\n","                    state_action_value += prob*reward+self.gamma*prob*self.table[next_state]\n","\n","                if best_state_action_value < state_action_value:\n","                    best_action = act\n","                    best_state_action_value = state_action_value\n","    \n","            policy_table[state] = best_action\n","\n","        self.policy = lambda obs: policy_table[obs]\n",""]},{"cell_type":"markdown","metadata":{},"source":[" Now we have built the Trainer class for policy iteration algorithm. In the following few cells, we will train the agent to solve the problem and evaluate its performance."]},{"cell_type":"code","execution_count":9,"metadata":{"tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":"[DEBUG]\tUpdated values for 200 steps. Difference between new and old table is: 5.930882294223811e-06\n[DEBUG]\tUpdated values for 400 steps. Difference between new and old table is: 5.1997225039568384e-08\n[DEBUG]\tUpdated values for 600 steps. Difference between new and old table is: 4.558920602137917e-10\n[INFO]\tIn 0 iteration, current mean episode reward is 0.335.\n[DEBUG]\tUpdated values for 200 steps. Difference between new and old table is: 0.043781313775536054\n[DEBUG]\tUpdated values for 400 steps. Difference between new and old table is: 0.005420149688518547\n[DEBUG]\tUpdated values for 600 steps. Difference between new and old table is: 0.0006487443111750715\n[DEBUG]\tUpdated values for 800 steps. Difference between new and old table is: 7.653453224701345e-05\n[DEBUG]\tUpdated values for 1000 steps. Difference between new and old table is: 8.971355053545571e-06\n[DEBUG]\tUpdated values for 1200 steps. Difference between new and old table is: 1.0485908991247905e-06\n[DEBUG]\tUpdated values for 1400 steps. Difference between new and old table is: 1.2240146204933744e-07\n[DEBUG]\tUpdated values for 1600 steps. Difference between new and old table is: 1.427937539044688e-08\n[DEBUG]\tUpdated values for 1800 steps. Difference between new and old table is: 1.6653839557401184e-09\n[DEBUG]\tUpdated values for 2000 steps. Difference between new and old table is: 1.9420747648934977e-10\n[INFO]\tIn 1 iteration, current mean episode reward is 0.773.\n[DEBUG]\tUpdated values for 200 steps. Difference between new and old table is: 5.9283194731404865e-05\n[DEBUG]\tUpdated values for 400 steps. Difference between new and old table is: 1.735662198687482e-08\n[INFO]\tIn 2 iteration, current mean episode reward is 0.875.\n[DEBUG]\tUpdated values for 200 steps. Difference between new and old table is: 0.00034379848869935115\n[DEBUG]\tUpdated values for 400 steps. Difference between new and old table is: 1.5461402430722027e-06\n[DEBUG]\tUpdated values for 600 steps. Difference between new and old table is: 6.96647714515386e-09\n[INFO]\tIn 3 iteration, current mean episode reward is 0.688.\n[DEBUG]\tUpdated values for 200 steps. Difference between new and old table is: 0.00026129581170659943\n[DEBUG]\tUpdated values for 400 steps. Difference between new and old table is: 1.4619278363880994e-05\n[DEBUG]\tUpdated values for 600 steps. Difference between new and old table is: 7.91412985090556e-07\n[DEBUG]\tUpdated values for 800 steps. Difference between new and old table is: 4.233956556187746e-08\n[DEBUG]\tUpdated values for 1000 steps. Difference between new and old table is: 2.255234193837552e-09\n[DEBUG]\tUpdated values for 1200 steps. Difference between new and old table is: 1.199296634224467e-10\n[INFO]\tIn 4 iteration, current mean episode reward is 0.87.\n[INFO]\tIn 5 iteration, current mean episode reward is 0.867.\nWe found policy is not changed anymore at itertaion 6. Current mean episode reward is 0.867. Stop training.\n"}],"source":["# Solve the TODOs and remove `pass`\n","\n","# Managing configurations of your experiments is important for your research.\n","default_pi_config = dict(\n","    max_iteration=1000,\n","    evaluate_interval=1,\n","    gamma=1.0,\n","    eps=1e-10\n",")\n","\n","\n","def policy_iteration(train_config=None):\n","    config = default_pi_config.copy()\n","    if train_config is not None:\n","        config.update(train_config)\n","        \n","    trainer = PolicyItertaionTrainer(gamma=config['gamma'], eps=config['eps'])\n","    old_policy_result = {\n","        obs: -1 for obs in range(trainer.obs_dim)\n","    }\n","    old_policy_result_func = lambda x: old_policy_result[x]\n","    for i in range(config['max_iteration']):\n","        # train the agent\n","        trainer.train()  # [TODO] please uncomment this line\n","\n","        # [TODO] compare the new policy with old policy to check whether\n","        #  should we stop. If new and old policy have same output given any\n","        #  observation, them we consider the algorithm is converged and\n","        #  should be stopped.\n","        should_stop = (sum([old_policy_result_func(obs) == trainer.policy(obs) for obs in range(trainer.obs_dim)]) == trainer.obs_dim)\n","        old_policy_result_func = trainer.policy\n","\n","        if should_stop:\n","            print(\"We found policy is not changed anymore at \"\n","                  \"itertaion {}. Current mean episode reward \"\n","                  \"is {}. Stop training.\".format(i, trainer.evaluate()))\n","            break\n","        \n","\n","        # evaluate the result\n","        if i % config['evaluate_interval'] == 0:\n","            print(\n","                \"[INFO]\\tIn {} iteration, current mean episode reward is {}.\"\n","                \"\".format(i, trainer.evaluate()))\n","\n","            if i > 20:\n","                print(\"You sure your codes is OK? It shouldn't take so many \"\n","                      \"({}) iterations to train a policy iteration \"\n","                      \"agent.\".format(i))\n","\n","    # assert trainer.evaluate() > 0.8,         \"We expect to get the mean episode reward greater than 0.8. \"         \"But you get: {}. Please check your codes.\".format(trainer.evaluate())\n","\n","    return trainer\n","\n","\n","# # %%\n","# # Run this cell without modification\n","\n","# # It may be confusing to call a trainer agent. But that's what we normally do.\n","pi_agent = policy_iteration()\n","\n",""]},{"cell_type":"code","execution_count":10,"metadata":{"tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":"Your policy iteration agent achieve 0.867 mean episode reward. The optimal score should be almost 0.86.\n"}],"source":["# Run this cell without modification\n","\n","print(\"Your policy iteration agent achieve {} mean episode reward. The optimal score \"\n","      \"should be almost {}.\".format(pi_agent.evaluate(), 0.86))\n","\n",""]},{"cell_type":"code","execution_count":11,"metadata":{"tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":"(Right)\nSFFFFFFF\nFFFFFFFF\nFFFHFFFF\nFFFFFHFF\nFFFHFFFF\nFHHFFFHF\nFHFFHFHF\nFFFHFFF\u001b[41mG\u001b[0m\n"}],"source":["# Run this cell without modification\n","\n","pi_agent.render()\n","\n",""]},{"cell_type":"code","execution_count":12,"metadata":{"tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":"+-----+-----+-----State Value Mapping-----+-----+-----+\n|     |   0 |   1 |   2 |   3 |   4 |   5 |   6 |   7 |\n|-----+-----+-----+-----+-----+-----+-----+-----+-----|\n| 0   |1.000|1.000|1.000|1.000|1.000|1.000|1.000|1.000|\n|     |     |     |     |     |     |     |     |     |\n+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n| 1   |1.000|1.000|1.000|1.000|1.000|1.000|1.000|1.000|\n|     |     |     |     |     |     |     |     |     |\n+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n| 2   |1.000|0.978|0.926|0.000|0.857|0.946|0.982|1.000|\n|     |     |     |     |     |     |     |     |     |\n+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n| 3   |1.000|0.935|0.801|0.475|0.624|0.000|0.945|1.000|\n|     |     |     |     |     |     |     |     |     |\n+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n| 4   |1.000|0.826|0.542|0.000|0.539|0.611|0.852|1.000|\n|     |     |     |     |     |     |     |     |     |\n+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n| 5   |1.000|0.000|0.000|0.168|0.383|0.442|0.000|1.000|\n|     |     |     |     |     |     |     |     |     |\n+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n| 6   |1.000|0.000|0.195|0.121|0.000|0.332|0.000|1.000|\n|     |     |     |     |     |     |     |     |     |\n+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n| 7   |1.000|0.732|0.463|0.000|0.277|0.555|0.777|0.000|\n|     |     |     |     |     |     |     |     |     |\n+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n\n"}],"source":["# Run this cell without modification\n","\n","pi_agent.print_table()\n",""]},{"cell_type":"markdown","metadata":{},"source":[" Congratulations! You have successfully implemented the policy iteration trainer (if and only if no error happens at the above cells). Few further problems for you to investigate:\n","\n"," 1. What is the impact of the discount factor gamma?\n"," 2. What is the impact of the value function convergence criterion epsilon?\n","\n"," If you are interested in doing more investigation (not limited to these two), feel free to open new cells at the end of this notebook and left a clear trace of your thinking and coding, which leads to extra credit if you do a good job. It's an optional job, and you can ignore it.\n","\n"," Now let's continue our journey!"]},{"cell_type":"markdown","metadata":{},"source":[" ### Section 2.2: Value Iteration\n","\n"," Recall the idea of value iteration. We update the state value:\n","\n"," $$v_{k+1}(s) = \\max_a E_{s'} [r(s, a) + \\gamma v_{k}(s')]$$\n","\n"," wherein the $s'$ is next state, $r$ is the reward, $v_{k}(s')$ is the next state value given by the old (not updated yet) value function. The expectation is computed among all possible transitions (given a state and action pair, it is possible to have many different next states, since the environment is not deterministic).\n","\n"," The value iteration algorithm does not require an inner loop. It computes the expected return of all possible actions at a given state and uses the maximum of them as the state value. You can imagine it \"pretends\" we already have the optimal policy and run policy iteration based on it. Therefore we do not need to maintain a policy object in a trainer. We only need to retrieve the optimal policy using the same rule as policy iteration, given current value function.\n","\n"," You should implement the trainer following the framework we already wrote for you. Please carefully go through the code and finish all `TODO` in it."]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["# Solve the TODOs and remove `pass`\n","\n","\n","class ValueIterationTrainer(PolicyItertaionTrainer):\n","    \"\"\"Note that we inherate Policy Iteration Trainer, to resue the\n","    code of update_policy(). It's same since it get optimal policy from\n","    current state-value table (self.table).\n","    \"\"\"\n","\n","    def __init__(self, gamma=1.0, env_name='FrozenLake8x8-v0'):\n","        super(ValueIterationTrainer, self).__init__(gamma, None, env_name)\n","\n","    def train(self):\n","        \"\"\"Conduct one iteration of learning.\"\"\"\n","        # [TODO] value function may be need to be reset to zeros.\n","        # if you think it should, than do it. If not, then move on.\n","        # pass\n","\n","        # In value iteration, we do not explicit require a\n","        # policy instance to run. We update value function\n","        # directly based on the transitions. Therefore, we\n","        # don't need to run self.update_policy() in each step.\n","        self.update_value_function()\n","\n","    def update_value_function(self):\n","        old_table = self.table.copy()\n","\n","        for state in range(self.obs_dim):\n","            state_value = 0\n","\n","            # [TODO] what should be de right state value?\n","            # hint: try to compute the state_action_values first\n","            \n","            state_action_values = [0]*self.action_dim\n","            for act in range(self.action_dim):\n","                transition_list = self._get_transitions(state, act)\n","                state_action_value =0\n","                for transition in transition_list:\n","                    prob = transition['prob']\n","                    reward = transition['reward']\n","                    next_state = transition['next_state']\n","                    done = transition['done']\n","                    state_action_value += reward+self.gamma*prob*old_table[next_state]\n","                    \n","                state_action_values[act] = state_action_value\n","            state_value = max(state_action_values)\n","                \n","\n","            self.table[state] = state_value\n","\n","        # Till now the one step value update is finished.\n","        # You can see that we do not use a inner loop to update\n","        # the value function like what we did in policy iteration.\n","        # This is because to compute the state value, which is\n","        # a expectation among all possible action given by a\n","        # specified policy, we **pretend** already own the optimal\n","        # policy (the max operation).\n","\n","    def evaluate(self):\n","        \"\"\"Since in value itertaion we do not maintain a policy function,\n","        so we need to retrieve it when we need it.\"\"\"\n","        self.update_policy()\n","        return super().evaluate()\n","\n","    def render(self):\n","        \"\"\"Since in value itertaion we do not maintain a policy function,\n","        so we need to retrieve it when we need it.\"\"\"\n","        self.update_policy()\n","        return super().render()\n","\n",""]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["# Solve the TODOs and remove `pass`\n","\n","# Managing configurations of your experiments is important for your research.\n","default_vi_config = dict(\n","    max_iteration=10000,\n","    evaluate_interval=100,  # don't need to update policy each iteration\n","    gamma=1.0,\n","    eps=1e-10\n",")\n","\n","\n","def value_iteration(train_config=None):\n","    config = default_vi_config.copy()\n","    if train_config is not None:\n","        config.update(train_config)\n","\n","    # [TODO] initialize Value Iteration Trainer. Remember to pass\n","    #  config['gamma'] to it.\n","    trainer = ValueIterationTrainer(gamma=config['gamma'])\n","\n","    # old_state_value_table = trainer.table.copy()\n","    old_policy_result = {\n","        obs: -1 for obs in range(trainer.obs_dim)\n","    }\n","    old_policy_result_func = lambda x: old_policy_result[x]\n","    patient = 0\n","    should_stop = False\n","    for i in range(config['max_iteration']):\n","        # train the agent\n","        trainer.train()  # [TODO] please uncomment this line\n","\n","        # evaluate the result\n","        if i % config['evaluate_interval'] == 0:\n","            print(\"[INFO]\\tIn {} iteration, current \"\n","                  \"mean episode reward is {}.\".format(\n","                i, trainer.evaluate()\n","            ))\n","\n","            # [TODO] compare the new policy with old policy to check should\n","            #  we stop.\n","            # [HINT] If new and old policy have same output given any\n","            #  observation, them we consider the algorithm is converged and\n","            #  should be stopped.\n","            patient  = patient + (sum([old_policy_result_func(obs) == trainer.policy(obs) for obs in range(trainer.obs_dim)]) == trainer.obs_dim)\n","            old_policy_result_func = trainer.policy\n","            if patient>3:\n","                should_stop = True\n","      \n","            \n","            if should_stop:\n","                print(\"We found policy is not changed anymore at \"\n","                      \"itertaion {}. Current mean episode reward \"\n","                      \"is {}. Stop training.\".format(i, trainer.evaluate()))\n","                break\n","\n","            if i > 3000:\n","                print(\"You sure your codes is OK? It shouldn't take so many \"\n","                      \"({}) iterations to train a policy iteration \"\n","                      \"agent.\".format(\n","                    i))\n","\n","    # assert trainer.evaluate() > 0.8,         \"We expect to get the mean episode reward greater than 0.8. \"         \"But you get: {}. Please check your codes.\".format(trainer.evaluate())\n","\n","    return trainer\n","\n",""]},{"cell_type":"code","execution_count":15,"metadata":{"tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":"[INFO]\tIn 0 iteration, current mean episode reward is 0.0.\n[INFO]\tIn 100 iteration, current mean episode reward is 0.892.\n[INFO]\tIn 200 iteration, current mean episode reward is 0.867.\n[INFO]\tIn 300 iteration, current mean episode reward is 0.867.\n[INFO]\tIn 400 iteration, current mean episode reward is 0.867.\n[INFO]\tIn 500 iteration, current mean episode reward is 0.867.\n[INFO]\tIn 600 iteration, current mean episode reward is 0.867.\n[INFO]\tIn 700 iteration, current mean episode reward is 0.867.\n[INFO]\tIn 800 iteration, current mean episode reward is 0.867.\nWe found policy is not changed anymore at itertaion 800. Current mean episode reward is 0.867. Stop training.\n"}],"source":["# Run this cell without modification\n","\n","vi_agent = value_iteration()\n","\n",""]},{"cell_type":"code","execution_count":16,"metadata":{"tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":"Your value iteration agent achieve 0.867 mean episode reward. The optimal score should be almost 0.86.\n"}],"source":["# Run this cell without modification\n","\n","print(\"Your value iteration agent achieve {} mean episode reward. The optimal score \"\n","      \"should be almost {}.\".format(vi_agent.evaluate(), 0.86))\n","\n",""]},{"cell_type":"code","execution_count":17,"metadata":{"tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":"(Right)\nSFFFFFFF\nFFFFFFFF\nFFFHFFFF\nFFFFFHFF\nFFFHFFFF\nFHHFFFHF\nFHFFHFHF\nFFFHFFF\u001b[41mG\u001b[0m\n"}],"source":["# Run this cell without modification\n","\n","vi_agent.render()\n","\n",""]},{"cell_type":"code","execution_count":18,"metadata":{"tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":"+-----+-----+-----State Value Mapping-----+-----+-----+\n|     |   0 |   1 |   2 |   3 |   4 |   5 |   6 |   7 |\n|-----+-----+-----+-----+-----+-----+-----+-----+-----|\n| 0   |3.000|3.000|3.000|3.000|3.000|3.000|3.000|3.000|\n|     |     |     |     |     |     |     |     |     |\n+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n| 1   |3.000|3.000|3.000|3.000|3.000|3.000|3.000|3.000|\n|     |     |     |     |     |     |     |     |     |\n+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n| 2   |3.000|2.935|2.779|0.000|2.570|2.839|2.946|3.000|\n|     |     |     |     |     |     |     |     |     |\n+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n| 3   |3.000|2.804|2.403|1.425|1.871|0.000|2.834|3.000|\n|     |     |     |     |     |     |     |     |     |\n+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n| 4   |3.000|2.477|1.627|0.000|1.618|1.834|2.556|3.000|\n|     |     |     |     |     |     |     |     |     |\n+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n| 5   |3.000|0.000|0.000|0.504|1.150|1.327|0.000|3.000|\n|     |     |     |     |     |     |     |     |     |\n+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n| 6   |3.000|0.000|0.584|0.363|0.000|0.997|0.000|3.000|\n|     |     |     |     |     |     |     |     |     |\n+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n| 7   |3.000|2.195|1.389|0.000|0.832|1.665|2.332|0.000|\n|     |     |     |     |     |     |     |     |     |\n+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n\n"}],"source":["# Run this cell without modification\n","\n","vi_agent.print_table()\n",""]},{"cell_type":"markdown","metadata":{},"source":[" Congratulation! You have successfully implemented the value iteration trainer (if and only if no error happens at the above cells). Few further problems for you to investigate:\n","\n"," 1. Do you see that some iteration during training yields better rewards than the final one?  Why does that happen?\n"," 2. What is the impact of the discount factor gamma?\n"," 3. What is the impact of the value function convergence criterion epsilon?\n","\n"," If you are interested in doing more investigation (not limited to these two), feel free to open new cells at the end of this notebook and left a clear trace of your thinking and coding, which leads to extra credit if you do a good job. It's an optional job, and you can ignore it.\n","\n"," Now let's continue our journey!"]},{"cell_type":"markdown","metadata":{},"source":[" ### Section 2.3: Compare two model-based agents\n","\n"," Now we have two agents: `pi_agent` and `vi_agent`. They are believed to be the optimal policy in this environment. Can you compare the policy of two of them and use a clean and clear description or figures to show your conclusion?"]},{"cell_type":"code","execution_count":19,"metadata":{"tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":"+-----+-----+-----State Value Mapping-----+-----+-----+\n|     |   0 |   1 |   2 |   3 |   4 |   5 |   6 |   7 |\n|-----+-----+-----+-----+-----+-----+-----+-----+-----|\n| 0   |3.000|3.000|3.000|3.000|3.000|3.000|3.000|3.000|\n|     |     |     |     |     |     |     |     |     |\n+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n| 1   |3.000|3.000|3.000|3.000|3.000|3.000|3.000|3.000|\n|     |     |     |     |     |     |     |     |     |\n+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n| 2   |3.000|2.935|2.779|0.000|2.570|2.839|2.946|3.000|\n|     |     |     |     |     |     |     |     |     |\n+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n| 3   |3.000|2.804|2.403|1.425|1.871|0.000|2.834|3.000|\n|     |     |     |     |     |     |     |     |     |\n+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n| 4   |3.000|2.477|1.627|0.000|1.618|1.834|2.556|3.000|\n|     |     |     |     |     |     |     |     |     |\n+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n| 5   |3.000|0.000|0.000|0.504|1.150|1.327|0.000|3.000|\n|     |     |     |     |     |     |     |     |     |\n+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n| 6   |3.000|0.000|0.584|0.363|0.000|0.997|0.000|3.000|\n|     |     |     |     |     |     |     |     |     |\n+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n| 7   |3.000|2.195|1.389|0.000|0.832|1.665|2.332|0.000|\n|     |     |     |     |     |     |     |     |     |\n+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n\n+-----+-----+-----State Value Mapping-----+-----+-----+\n|     |   0 |   1 |   2 |   3 |   4 |   5 |   6 |   7 |\n|-----+-----+-----+-----+-----+-----+-----+-----+-----|\n| 0   |3.000|3.000|3.000|3.000|3.000|3.000|3.000|3.000|\n|     |     |     |     |     |     |     |     |     |\n+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n| 1   |3.000|3.000|3.000|3.000|3.000|3.000|3.000|3.000|\n|     |     |     |     |     |     |     |     |     |\n+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n| 2   |3.000|2.935|2.779|0.000|2.570|2.839|2.946|3.000|\n|     |     |     |     |     |     |     |     |     |\n+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n| 3   |3.000|2.804|2.403|1.425|1.871|0.000|2.834|3.000|\n|     |     |     |     |     |     |     |     |     |\n+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n| 4   |3.000|2.477|1.627|0.000|1.618|1.834|2.556|3.000|\n|     |     |     |     |     |     |     |     |     |\n+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n| 5   |3.000|0.000|0.000|0.504|1.150|1.327|0.000|3.000|\n|     |     |     |     |     |     |     |     |     |\n+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n| 6   |3.000|0.000|0.584|0.363|0.000|0.997|0.000|3.000|\n|     |     |     |     |     |     |     |     |     |\n+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n| 7   |3.000|2.195|1.389|0.000|0.832|1.665|2.332|0.000|\n|     |     |     |     |     |     |     |     |     |\n+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n\n"}],"source":["# Solve the TODO and remove `pass`\n","\n","# [TODO] try to compare two trained agents' policies\n","# hint: trainer.print_table() may give you a better sense.\n","vi_agent.print_table()\n","pi_agent.table = pi_agent.table*3\n","pi_agent.print_table()"]},{"cell_type":"markdown","metadata":{},"source":[" I think the value iteration agents value table is 3 times larger than the policy iteration agent's value table\n"," If I multiply the pi_agent.table by 3 the result is the same as the vi_agent.table!"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["# You can do more inverstigation here if you wish. Leave it blank if you don't.\n",""]},{"cell_type":"markdown","metadata":{},"source":[" 1. Do you see that some iteration during training yields better rewards than the final one?  Why does that happen?\n","       \n","       Yes I found it happen frequently.\n","       I think the randomness of the model evaluation cause it.\n","       with the random start with given seed we may reach the goal with high rewards\n","       However, It may not cover all possibility of the given environment.\n","       so it just a local optimal of the given seed.\n","       Not the optimal policy of this enviroment setting.\n","\n"," 2. What is the impact of the discount factor gamma?\n","       \n","       Using the grid search we can find that the higher $\\gamma$ will boost the performance of the model.\n","       since model will gain farsight by setting the high  $\\gamma$ value.\n"]},{"cell_type":"code","execution_count":21,"metadata":{"tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":"current gamma:0.0\n[INFO]\tIn 0 iteration, current mean episode reward is 0.0.\nWe found policy is not changed anymore at itertaion 1. Current mean episode reward is 0.0. Stop training.\ncurrent gamma:0.1\n[INFO]\tIn 0 iteration, current mean episode reward is 0.0.\n[INFO]\tIn 1 iteration, current mean episode reward is 0.445.\n[INFO]\tIn 2 iteration, current mean episode reward is 0.507.\n[INFO]\tIn 3 iteration, current mean episode reward is 0.51.\n[INFO]\tIn 4 iteration, current mean episode reward is 0.503.\n[INFO]\tIn 5 iteration, current mean episode reward is 0.507.\nWe found policy is not changed anymore at itertaion 6. Current mean episode reward is 0.507. Stop training.\ncurrent gamma:0.2\n[INFO]\tIn 0 iteration, current mean episode reward is 0.379.\n[INFO]\tIn 1 iteration, current mean episode reward is 0.507.\n[INFO]\tIn 2 iteration, current mean episode reward is 0.551.\n[INFO]\tIn 3 iteration, current mean episode reward is 0.552.\nWe found policy is not changed anymore at itertaion 4. Current mean episode reward is 0.552. Stop training.\ncurrent gamma:0.3\n[INFO]\tIn 0 iteration, current mean episode reward is 0.0.\n[INFO]\tIn 1 iteration, current mean episode reward is 0.0.\n[INFO]\tIn 2 iteration, current mean episode reward is 0.0.\n[INFO]\tIn 3 iteration, current mean episode reward is 0.0.\n[INFO]\tIn 4 iteration, current mean episode reward is 0.0.\n[INFO]\tIn 5 iteration, current mean episode reward is 0.0.\n[INFO]\tIn 6 iteration, current mean episode reward is 0.451.\n[INFO]\tIn 7 iteration, current mean episode reward is 0.551.\nWe found policy is not changed anymore at itertaion 8. Current mean episode reward is 0.551. Stop training.\ncurrent gamma:0.4\n[INFO]\tIn 0 iteration, current mean episode reward is 0.366.\n[INFO]\tIn 1 iteration, current mean episode reward is 0.478.\n[INFO]\tIn 2 iteration, current mean episode reward is 0.551.\n[INFO]\tIn 3 iteration, current mean episode reward is 0.551.\nWe found policy is not changed anymore at itertaion 4. Current mean episode reward is 0.551. Stop training.\ncurrent gamma:0.5\n[INFO]\tIn 0 iteration, current mean episode reward is 0.63.\n[INFO]\tIn 1 iteration, current mean episode reward is 0.596.\n[INFO]\tIn 2 iteration, current mean episode reward is 0.551.\n[INFO]\tIn 3 iteration, current mean episode reward is 0.552.\n[INFO]\tIn 4 iteration, current mean episode reward is 0.551.\nWe found policy is not changed anymore at itertaion 5. Current mean episode reward is 0.551. Stop training.\ncurrent gamma:0.6\n[INFO]\tIn 0 iteration, current mean episode reward is 0.0.\n[INFO]\tIn 1 iteration, current mean episode reward is 0.0.\n[INFO]\tIn 2 iteration, current mean episode reward is 0.0.\n[INFO]\tIn 3 iteration, current mean episode reward is 0.0.\n[INFO]\tIn 4 iteration, current mean episode reward is 0.0.\n[INFO]\tIn 5 iteration, current mean episode reward is 0.0.\n[INFO]\tIn 6 iteration, current mean episode reward is 0.47.\n[INFO]\tIn 7 iteration, current mean episode reward is 0.551.\nWe found policy is not changed anymore at itertaion 8. Current mean episode reward is 0.551. Stop training.\ncurrent gamma:0.7\n[INFO]\tIn 0 iteration, current mean episode reward is 0.0.\n[INFO]\tIn 1 iteration, current mean episode reward is 0.0.\n[INFO]\tIn 2 iteration, current mean episode reward is 0.0.\n[INFO]\tIn 3 iteration, current mean episode reward is 0.0.\n[INFO]\tIn 4 iteration, current mean episode reward is 0.0.\n[INFO]\tIn 5 iteration, current mean episode reward is 0.47.\n[INFO]\tIn 6 iteration, current mean episode reward is 0.597.\n[INFO]\tIn 7 iteration, current mean episode reward is 0.598.\nWe found policy is not changed anymore at itertaion 8. Current mean episode reward is 0.598. Stop training.\ncurrent gamma:0.8\n[INFO]\tIn 0 iteration, current mean episode reward is 0.0.\n[INFO]\tIn 1 iteration, current mean episode reward is 0.0.\n[INFO]\tIn 2 iteration, current mean episode reward is 0.0.\n[INFO]\tIn 3 iteration, current mean episode reward is 0.0.\n[INFO]\tIn 4 iteration, current mean episode reward is 0.0.\n[INFO]\tIn 5 iteration, current mean episode reward is 0.47.\n[INFO]\tIn 6 iteration, current mean episode reward is 0.597.\n[INFO]\tIn 7 iteration, current mean episode reward is 0.645.\nWe found policy is not changed anymore at itertaion 8. Current mean episode reward is 0.645. Stop training.\ncurrent gamma:0.9\n[INFO]\tIn 0 iteration, current mean episode reward is 0.0.\n[INFO]\tIn 1 iteration, current mean episode reward is 0.0.\n[INFO]\tIn 2 iteration, current mean episode reward is 0.0.\n[INFO]\tIn 3 iteration, current mean episode reward is 0.0.\n[INFO]\tIn 4 iteration, current mean episode reward is 0.0.\n[INFO]\tIn 5 iteration, current mean episode reward is 0.47.\n[INFO]\tIn 6 iteration, current mean episode reward is 0.67.\n[INFO]\tIn 7 iteration, current mean episode reward is 0.718.\nWe found policy is not changed anymore at itertaion 8. Current mean episode reward is 0.718. Stop training.\ncurrent gamma:1.0\n[DEBUG]\tUpdated values for 200 steps. Difference between new and old table is: 4.029768152318298e-05\n[DEBUG]\tUpdated values for 400 steps. Difference between new and old table is: 4.671516246733279e-07\n[DEBUG]\tUpdated values for 600 steps. Difference between new and old table is: 5.415571866087621e-09\n[INFO]\tIn 0 iteration, current mean episode reward is 0.413.\n[DEBUG]\tUpdated values for 200 steps. Difference between new and old table is: 0.003303443345733091\n[DEBUG]\tUpdated values for 400 steps. Difference between new and old table is: 1.4873287253750617e-05\n[DEBUG]\tUpdated values for 600 steps. Difference between new and old table is: 6.696042477091768e-08\n[DEBUG]\tUpdated values for 800 steps. Difference between new and old table is: 3.014598570771909e-10\n[INFO]\tIn 1 iteration, current mean episode reward is 0.806.\n[DEBUG]\tUpdated values for 200 steps. Difference between new and old table is: 0.02550714501799789\n[DEBUG]\tUpdated values for 400 steps. Difference between new and old table is: 0.0014653672088468378\n[DEBUG]\tUpdated values for 600 steps. Difference between new and old table is: 7.83853761475023e-05\n[DEBUG]\tUpdated values for 800 steps. Difference between new and old table is: 4.166922333306844e-06\n[DEBUG]\tUpdated values for 1000 steps. Difference between new and old table is: 2.2138540914373728e-07\n[DEBUG]\tUpdated values for 1200 steps. Difference between new and old table is: 1.176142720715756e-08\n[DEBUG]\tUpdated values for 1400 steps. Difference between new and old table is: 6.248396383634613e-10\n[INFO]\tIn 2 iteration, current mean episode reward is 0.77.\n[DEBUG]\tUpdated values for 200 steps. Difference between new and old table is: 0.00018036258444149855\n[DEBUG]\tUpdated values for 400 steps. Difference between new and old table is: 3.62801983508354e-06\n[DEBUG]\tUpdated values for 600 steps. Difference between new and old table is: 7.30239434754143e-08\n[DEBUG]\tUpdated values for 800 steps. Difference between new and old table is: 1.4698085110120829e-09\n[INFO]\tIn 3 iteration, current mean episode reward is 0.688.\n[DEBUG]\tUpdated values for 200 steps. Difference between new and old table is: 0.0001289797785377772\n[DEBUG]\tUpdated values for 400 steps. Difference between new and old table is: 6.465502466787565e-06\n[DEBUG]\tUpdated values for 600 steps. Difference between new and old table is: 3.433698128330054e-07\n[DEBUG]\tUpdated values for 800 steps. Difference between new and old table is: 1.824189743704352e-08\n[DEBUG]\tUpdated values for 1000 steps. Difference between new and old table is: 9.691236285691573e-10\n[INFO]\tIn 4 iteration, current mean episode reward is 0.869.\n[DEBUG]\tUpdated values for 200 steps. Difference between new and old table is: 4.6363333258914174e-05\n[DEBUG]\tUpdated values for 400 steps. Difference between new and old table is: 2.1261451699228218e-07\n[DEBUG]\tUpdated values for 600 steps. Difference between new and old table is: 9.750154017496016e-10\n[INFO]\tIn 5 iteration, current mean episode reward is 0.863.\n[DEBUG]\tUpdated values for 200 steps. Difference between new and old table is: 6.185307305646426e-05\n[DEBUG]\tUpdated values for 400 steps. Difference between new and old table is: 1.2365406895364917e-06\n[DEBUG]\tUpdated values for 600 steps. Difference between new and old table is: 2.47204029052428e-08\n[DEBUG]\tUpdated values for 800 steps. Difference between new and old table is: 4.941999726115753e-10\n[INFO]\tIn 6 iteration, current mean episode reward is 0.854.\nWe found policy is not changed anymore at itertaion 7. Current mean episode reward is 0.854. Stop training.\ncurrent gamma:0.0\n[INFO]\tIn 0 iteration, current mean episode reward is 0.0.\n[INFO]\tIn 100 iteration, current mean episode reward is 0.0.\n[INFO]\tIn 200 iteration, current mean episode reward is 0.0.\n[INFO]\tIn 300 iteration, current mean episode reward is 0.0.\n[INFO]\tIn 400 iteration, current mean episode reward is 0.0.\nWe found policy is not changed anymore at itertaion 400. Current mean episode reward is 0.0. Stop training.\ncurrent gamma:0.1\n[INFO]\tIn 0 iteration, current mean episode reward is 0.0.\n[INFO]\tIn 100 iteration, current mean episode reward is 0.507.\n[INFO]\tIn 200 iteration, current mean episode reward is 0.507.\n[INFO]\tIn 300 iteration, current mean episode reward is 0.507.\n[INFO]\tIn 400 iteration, current mean episode reward is 0.507.\n[INFO]\tIn 500 iteration, current mean episode reward is 0.507.\nWe found policy is not changed anymore at itertaion 500. Current mean episode reward is 0.507. Stop training.\ncurrent gamma:0.2\n[INFO]\tIn 0 iteration, current mean episode reward is 0.0.\n[INFO]\tIn 100 iteration, current mean episode reward is 0.551.\n[INFO]\tIn 200 iteration, current mean episode reward is 0.551.\n[INFO]\tIn 300 iteration, current mean episode reward is 0.551.\n[INFO]\tIn 400 iteration, current mean episode reward is 0.551.\n[INFO]\tIn 500 iteration, current mean episode reward is 0.551.\nWe found policy is not changed anymore at itertaion 500. Current mean episode reward is 0.551. Stop training.\ncurrent gamma:0.3\n[INFO]\tIn 0 iteration, current mean episode reward is 0.0.\n[INFO]\tIn 100 iteration, current mean episode reward is 0.551.\n[INFO]\tIn 200 iteration, current mean episode reward is 0.551.\n[INFO]\tIn 300 iteration, current mean episode reward is 0.551.\n[INFO]\tIn 400 iteration, current mean episode reward is 0.551.\n[INFO]\tIn 500 iteration, current mean episode reward is 0.551.\nWe found policy is not changed anymore at itertaion 500. Current mean episode reward is 0.551. Stop training.\ncurrent gamma:0.4\n[INFO]\tIn 0 iteration, current mean episode reward is 0.0.\n[INFO]\tIn 100 iteration, current mean episode reward is 0.551.\n[INFO]\tIn 200 iteration, current mean episode reward is 0.551.\n[INFO]\tIn 300 iteration, current mean episode reward is 0.551.\n[INFO]\tIn 400 iteration, current mean episode reward is 0.551.\n[INFO]\tIn 500 iteration, current mean episode reward is 0.551.\nWe found policy is not changed anymore at itertaion 500. Current mean episode reward is 0.551. Stop training.\ncurrent gamma:0.5\n[INFO]\tIn 0 iteration, current mean episode reward is 0.0.\n[INFO]\tIn 100 iteration, current mean episode reward is 0.551.\n[INFO]\tIn 200 iteration, current mean episode reward is 0.551.\n[INFO]\tIn 300 iteration, current mean episode reward is 0.551.\n[INFO]\tIn 400 iteration, current mean episode reward is 0.551.\n[INFO]\tIn 500 iteration, current mean episode reward is 0.551.\nWe found policy is not changed anymore at itertaion 500. Current mean episode reward is 0.551. Stop training.\ncurrent gamma:0.6\n[INFO]\tIn 0 iteration, current mean episode reward is 0.0.\n[INFO]\tIn 100 iteration, current mean episode reward is 0.551.\n[INFO]\tIn 200 iteration, current mean episode reward is 0.551.\n[INFO]\tIn 300 iteration, current mean episode reward is 0.551.\n[INFO]\tIn 400 iteration, current mean episode reward is 0.551.\n[INFO]\tIn 500 iteration, current mean episode reward is 0.551.\nWe found policy is not changed anymore at itertaion 500. Current mean episode reward is 0.551. Stop training.\ncurrent gamma:0.7\n[INFO]\tIn 0 iteration, current mean episode reward is 0.0.\n[INFO]\tIn 100 iteration, current mean episode reward is 0.598.\n[INFO]\tIn 200 iteration, current mean episode reward is 0.598.\n[INFO]\tIn 300 iteration, current mean episode reward is 0.598.\n[INFO]\tIn 400 iteration, current mean episode reward is 0.598.\n[INFO]\tIn 500 iteration, current mean episode reward is 0.598.\nWe found policy is not changed anymore at itertaion 500. Current mean episode reward is 0.598. Stop training.\ncurrent gamma:0.8\n[INFO]\tIn 0 iteration, current mean episode reward is 0.0.\n[INFO]\tIn 100 iteration, current mean episode reward is 0.645.\n[INFO]\tIn 200 iteration, current mean episode reward is 0.645.\n[INFO]\tIn 300 iteration, current mean episode reward is 0.645.\n[INFO]\tIn 400 iteration, current mean episode reward is 0.645.\n[INFO]\tIn 500 iteration, current mean episode reward is 0.645.\nWe found policy is not changed anymore at itertaion 500. Current mean episode reward is 0.645. Stop training.\ncurrent gamma:0.9\n[INFO]\tIn 0 iteration, current mean episode reward is 0.0.\n[INFO]\tIn 100 iteration, current mean episode reward is 0.718.\n[INFO]\tIn 200 iteration, current mean episode reward is 0.718.\n[INFO]\tIn 300 iteration, current mean episode reward is 0.718.\n[INFO]\tIn 400 iteration, current mean episode reward is 0.718.\n[INFO]\tIn 500 iteration, current mean episode reward is 0.718.\nWe found policy is not changed anymore at itertaion 500. Current mean episode reward is 0.718. Stop training.\ncurrent gamma:1.0\n[INFO]\tIn 0 iteration, current mean episode reward is 0.0.\n[INFO]\tIn 100 iteration, current mean episode reward is 0.892.\n[INFO]\tIn 200 iteration, current mean episode reward is 0.867.\n[INFO]\tIn 300 iteration, current mean episode reward is 0.867.\n[INFO]\tIn 400 iteration, current mean episode reward is 0.867.\n[INFO]\tIn 500 iteration, current mean episode reward is 0.867.\n[INFO]\tIn 600 iteration, current mean episode reward is 0.867.\n[INFO]\tIn 700 iteration, current mean episode reward is 0.867.\n[INFO]\tIn 800 iteration, current mean episode reward is 0.867.\nWe found policy is not changed anymore at itertaion 800. Current mean episode reward is 0.867. Stop training.\n"}],"source":["rewards_pi = []\n","for gamma in range(11):\n","    gamma =gamma/10.0\n","    curr_config = default_pi_config.copy()\n","    curr_config['gamma'] = gamma\n","    print('current gamma:{}'.format(gamma))\n","    pi_agent = policy_iteration(curr_config)\n","    rewards_pi.append(pi_agent.evaluate())\n","\n","rewards_vi = []\n","for gamma in range(11):\n","    gamma =gamma/10.0\n","    curr_config = default_vi_config.copy()\n","    curr_config['gamma'] = gamma\n","    print('current gamma:{}'.format(gamma))\n","    vi_agent = value_iteration(curr_config)\n","    rewards_vi.append(vi_agent.evaluate())\n"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 864x288 with 2 Axes>","image/svg+xml":"<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"279.274375pt\" version=\"1.1\" viewBox=\"0 0 856.50625 279.274375\" width=\"856.50625pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2020-09-23T19:44:52.875099</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.3.2, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 279.274375 \nL 856.50625 279.274375 \nL 856.50625 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 43.78125 241.718125 \nL 422.70625 241.718125 \nL 422.70625 22.318125 \nL 43.78125 22.318125 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path clip-path=\"url(#p3281c39a5f)\" d=\"M 61.005114 241.718125 \nL 61.005114 22.318125 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_2\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"ma74f7f3276\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"61.005114\" xlink:href=\"#ma74f7f3276\" y=\"241.718125\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0.0 -->\n      <g transform=\"translate(53.053551 256.316563)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n        <path d=\"M 10.6875 12.40625 \nL 21 12.40625 \nL 21 0 \nL 10.6875 0 \nz\n\" id=\"DejaVuSans-46\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_3\">\n      <path clip-path=\"url(#p3281c39a5f)\" d=\"M 129.900568 241.718125 \nL 129.900568 22.318125 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"129.900568\" xlink:href=\"#ma74f7f3276\" y=\"241.718125\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 0.2 -->\n      <g transform=\"translate(121.949006 256.316563)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_5\">\n      <path clip-path=\"url(#p3281c39a5f)\" d=\"M 198.796023 241.718125 \nL 198.796023 22.318125 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"198.796023\" xlink:href=\"#ma74f7f3276\" y=\"241.718125\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 0.4 -->\n      <g transform=\"translate(190.84446 256.316563)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_7\">\n      <path clip-path=\"url(#p3281c39a5f)\" d=\"M 267.691477 241.718125 \nL 267.691477 22.318125 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"267.691477\" xlink:href=\"#ma74f7f3276\" y=\"241.718125\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 0.6 -->\n      <g transform=\"translate(259.739915 256.316563)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_9\">\n      <path clip-path=\"url(#p3281c39a5f)\" d=\"M 336.586932 241.718125 \nL 336.586932 22.318125 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"336.586932\" xlink:href=\"#ma74f7f3276\" y=\"241.718125\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 0.8 -->\n      <g transform=\"translate(328.635369 256.316563)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_11\">\n      <path clip-path=\"url(#p3281c39a5f)\" d=\"M 405.482386 241.718125 \nL 405.482386 22.318125 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"405.482386\" xlink:href=\"#ma74f7f3276\" y=\"241.718125\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 1.0 -->\n      <g transform=\"translate(397.530824 256.316563)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_7\">\n     <!-- Gamma -->\n     <g transform=\"translate(213.500781 269.994688)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 59.515625 10.40625 \nL 59.515625 29.984375 \nL 43.40625 29.984375 \nL 43.40625 38.09375 \nL 69.28125 38.09375 \nL 69.28125 6.78125 \nQ 63.578125 2.734375 56.6875 0.65625 \nQ 49.8125 -1.421875 42 -1.421875 \nQ 24.90625 -1.421875 15.25 8.5625 \nQ 5.609375 18.5625 5.609375 36.375 \nQ 5.609375 54.25 15.25 64.234375 \nQ 24.90625 74.21875 42 74.21875 \nQ 49.125 74.21875 55.546875 72.453125 \nQ 61.96875 70.703125 67.390625 67.28125 \nL 67.390625 56.78125 \nQ 61.921875 61.421875 55.765625 63.765625 \nQ 49.609375 66.109375 42.828125 66.109375 \nQ 29.4375 66.109375 22.71875 58.640625 \nQ 16.015625 51.171875 16.015625 36.375 \nQ 16.015625 21.625 22.71875 14.15625 \nQ 29.4375 6.6875 42.828125 6.6875 \nQ 48.046875 6.6875 52.140625 7.59375 \nQ 56.25 8.5 59.515625 10.40625 \nz\n\" id=\"DejaVuSans-71\"/>\n       <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n       <path d=\"M 52 44.1875 \nQ 55.375 50.25 60.0625 53.125 \nQ 64.75 56 71.09375 56 \nQ 79.640625 56 84.28125 50.015625 \nQ 88.921875 44.046875 88.921875 33.015625 \nL 88.921875 0 \nL 79.890625 0 \nL 79.890625 32.71875 \nQ 79.890625 40.578125 77.09375 44.375 \nQ 74.3125 48.1875 68.609375 48.1875 \nQ 61.625 48.1875 57.5625 43.546875 \nQ 53.515625 38.921875 53.515625 30.90625 \nL 53.515625 0 \nL 44.484375 0 \nL 44.484375 32.71875 \nQ 44.484375 40.625 41.703125 44.40625 \nQ 38.921875 48.1875 33.109375 48.1875 \nQ 26.21875 48.1875 22.15625 43.53125 \nQ 18.109375 38.875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.1875 51.21875 25.484375 53.609375 \nQ 29.78125 56 35.6875 56 \nQ 41.65625 56 45.828125 52.96875 \nQ 50 49.953125 52 44.1875 \nz\n\" id=\"DejaVuSans-109\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-71\"/>\n      <use x=\"77.490234\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"138.769531\" xlink:href=\"#DejaVuSans-109\"/>\n      <use x=\"236.181641\" xlink:href=\"#DejaVuSans-109\"/>\n      <use x=\"333.59375\" xlink:href=\"#DejaVuSans-97\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_13\">\n      <path clip-path=\"url(#p3281c39a5f)\" d=\"M 43.78125 231.745398 \nL 422.70625 231.745398 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_14\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m65960bc0ca\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m65960bc0ca\" y=\"231.745398\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0.0 -->\n      <g transform=\"translate(20.878125 235.544616)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_15\">\n      <path clip-path=\"url(#p3281c39a5f)\" d=\"M 43.78125 185.034731 \nL 422.70625 185.034731 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m65960bc0ca\" y=\"185.034731\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0.2 -->\n      <g transform=\"translate(20.878125 188.83395)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_17\">\n      <path clip-path=\"url(#p3281c39a5f)\" d=\"M 43.78125 138.324065 \nL 422.70625 138.324065 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_18\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m65960bc0ca\" y=\"138.324065\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0.4 -->\n      <g transform=\"translate(20.878125 142.123284)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_19\">\n      <path clip-path=\"url(#p3281c39a5f)\" d=\"M 43.78125 91.613399 \nL 422.70625 91.613399 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_20\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m65960bc0ca\" y=\"91.613399\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 0.6 -->\n      <g transform=\"translate(20.878125 95.412617)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_21\">\n      <path clip-path=\"url(#p3281c39a5f)\" d=\"M 43.78125 44.902732 \nL 422.70625 44.902732 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_22\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#m65960bc0ca\" y=\"44.902732\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 0.8 -->\n      <g transform=\"translate(20.878125 48.701951)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_13\">\n     <!-- Mean rewards -->\n     <g transform=\"translate(14.798438 167.150156)rotate(-90)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 9.8125 72.90625 \nL 24.515625 72.90625 \nL 43.109375 23.296875 \nL 61.8125 72.90625 \nL 76.515625 72.90625 \nL 76.515625 0 \nL 66.890625 0 \nL 66.890625 64.015625 \nL 48.09375 14.015625 \nL 38.1875 14.015625 \nL 19.390625 64.015625 \nL 19.390625 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-77\"/>\n       <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n       <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n       <path id=\"DejaVuSans-32\"/>\n       <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n       <path d=\"M 4.203125 54.6875 \nL 13.1875 54.6875 \nL 24.421875 12.015625 \nL 35.59375 54.6875 \nL 46.1875 54.6875 \nL 57.421875 12.015625 \nL 68.609375 54.6875 \nL 77.59375 54.6875 \nL 63.28125 0 \nL 52.6875 0 \nL 40.921875 44.828125 \nL 29.109375 0 \nL 18.5 0 \nz\n\" id=\"DejaVuSans-119\"/>\n       <path d=\"M 45.40625 46.390625 \nL 45.40625 75.984375 \nL 54.390625 75.984375 \nL 54.390625 0 \nL 45.40625 0 \nL 45.40625 8.203125 \nQ 42.578125 3.328125 38.25 0.953125 \nQ 33.9375 -1.421875 27.875 -1.421875 \nQ 17.96875 -1.421875 11.734375 6.484375 \nQ 5.515625 14.40625 5.515625 27.296875 \nQ 5.515625 40.1875 11.734375 48.09375 \nQ 17.96875 56 27.875 56 \nQ 33.9375 56 38.25 53.625 \nQ 42.578125 51.265625 45.40625 46.390625 \nz\nM 14.796875 27.296875 \nQ 14.796875 17.390625 18.875 11.75 \nQ 22.953125 6.109375 30.078125 6.109375 \nQ 37.203125 6.109375 41.296875 11.75 \nQ 45.40625 17.390625 45.40625 27.296875 \nQ 45.40625 37.203125 41.296875 42.84375 \nQ 37.203125 48.484375 30.078125 48.484375 \nQ 22.953125 48.484375 18.875 42.84375 \nQ 14.796875 37.203125 14.796875 27.296875 \nz\n\" id=\"DejaVuSans-100\"/>\n       <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-77\"/>\n      <use x=\"86.279297\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"147.802734\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"209.082031\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"272.460938\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"304.248047\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"343.111328\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"404.634766\" xlink:href=\"#DejaVuSans-119\"/>\n      <use x=\"486.421875\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"547.701172\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"587.064453\" xlink:href=\"#DejaVuSans-100\"/>\n      <use x=\"650.541016\" xlink:href=\"#DejaVuSans-115\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_23\">\n    <path clip-path=\"url(#p3281c39a5f)\" d=\"M 61.005114 231.745398 \nL 95.452841 113.333858 \nL 129.900568 102.823959 \nL 164.348295 103.057512 \nL 198.796023 103.057512 \nL 233.24375 103.057512 \nL 267.691477 103.057512 \nL 302.139205 92.080505 \nL 336.586932 81.103499 \nL 371.034659 64.054105 \nL 405.482386 32.290852 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 43.78125 241.718125 \nL 43.78125 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 422.70625 241.718125 \nL 422.70625 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 43.78125 241.718125 \nL 422.70625 241.718125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 43.78125 22.318125 \nL 422.70625 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_14\">\n    <!-- Police iteration performance vs gamma -->\n    <g transform=\"translate(114.917188 16.318125)scale(0.12 -0.12)\">\n     <defs>\n      <path d=\"M 19.671875 64.796875 \nL 19.671875 37.40625 \nL 32.078125 37.40625 \nQ 38.96875 37.40625 42.71875 40.96875 \nQ 46.484375 44.53125 46.484375 51.125 \nQ 46.484375 57.671875 42.71875 61.234375 \nQ 38.96875 64.796875 32.078125 64.796875 \nz\nM 9.8125 72.90625 \nL 32.078125 72.90625 \nQ 44.34375 72.90625 50.609375 67.359375 \nQ 56.890625 61.8125 56.890625 51.125 \nQ 56.890625 40.328125 50.609375 34.8125 \nQ 44.34375 29.296875 32.078125 29.296875 \nL 19.671875 29.296875 \nL 19.671875 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-80\"/>\n      <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n      <path d=\"M 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 0 \nL 9.421875 0 \nz\n\" id=\"DejaVuSans-108\"/>\n      <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n      <path d=\"M 48.78125 52.59375 \nL 48.78125 44.1875 \nQ 44.96875 46.296875 41.140625 47.34375 \nQ 37.3125 48.390625 33.40625 48.390625 \nQ 24.65625 48.390625 19.8125 42.84375 \nQ 14.984375 37.3125 14.984375 27.296875 \nQ 14.984375 17.28125 19.8125 11.734375 \nQ 24.65625 6.203125 33.40625 6.203125 \nQ 37.3125 6.203125 41.140625 7.25 \nQ 44.96875 8.296875 48.78125 10.40625 \nL 48.78125 2.09375 \nQ 45.015625 0.34375 40.984375 -0.53125 \nQ 36.96875 -1.421875 32.421875 -1.421875 \nQ 20.0625 -1.421875 12.78125 6.34375 \nQ 5.515625 14.109375 5.515625 27.296875 \nQ 5.515625 40.671875 12.859375 48.328125 \nQ 20.21875 56 33.015625 56 \nQ 37.15625 56 41.109375 55.140625 \nQ 45.0625 54.296875 48.78125 52.59375 \nz\n\" id=\"DejaVuSans-99\"/>\n      <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n      <path d=\"M 18.109375 8.203125 \nL 18.109375 -20.796875 \nL 9.078125 -20.796875 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nz\nM 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\n\" id=\"DejaVuSans-112\"/>\n      <path d=\"M 37.109375 75.984375 \nL 37.109375 68.5 \nL 28.515625 68.5 \nQ 23.6875 68.5 21.796875 66.546875 \nQ 19.921875 64.59375 19.921875 59.515625 \nL 19.921875 54.6875 \nL 34.71875 54.6875 \nL 34.71875 47.703125 \nL 19.921875 47.703125 \nL 19.921875 0 \nL 10.890625 0 \nL 10.890625 47.703125 \nL 2.296875 47.703125 \nL 2.296875 54.6875 \nL 10.890625 54.6875 \nL 10.890625 58.5 \nQ 10.890625 67.625 15.140625 71.796875 \nQ 19.390625 75.984375 28.609375 75.984375 \nz\n\" id=\"DejaVuSans-102\"/>\n      <path d=\"M 2.984375 54.6875 \nL 12.5 54.6875 \nL 29.59375 8.796875 \nL 46.6875 54.6875 \nL 56.203125 54.6875 \nL 35.6875 0 \nL 23.484375 0 \nz\n\" id=\"DejaVuSans-118\"/>\n      <path d=\"M 45.40625 27.984375 \nQ 45.40625 37.75 41.375 43.109375 \nQ 37.359375 48.484375 30.078125 48.484375 \nQ 22.859375 48.484375 18.828125 43.109375 \nQ 14.796875 37.75 14.796875 27.984375 \nQ 14.796875 18.265625 18.828125 12.890625 \nQ 22.859375 7.515625 30.078125 7.515625 \nQ 37.359375 7.515625 41.375 12.890625 \nQ 45.40625 18.265625 45.40625 27.984375 \nz\nM 54.390625 6.78125 \nQ 54.390625 -7.171875 48.1875 -13.984375 \nQ 42 -20.796875 29.203125 -20.796875 \nQ 24.46875 -20.796875 20.265625 -20.09375 \nQ 16.0625 -19.390625 12.109375 -17.921875 \nL 12.109375 -9.1875 \nQ 16.0625 -11.328125 19.921875 -12.34375 \nQ 23.78125 -13.375 27.78125 -13.375 \nQ 36.625 -13.375 41.015625 -8.765625 \nQ 45.40625 -4.15625 45.40625 5.171875 \nL 45.40625 9.625 \nQ 42.625 4.78125 38.28125 2.390625 \nQ 33.9375 0 27.875 0 \nQ 17.828125 0 11.671875 7.65625 \nQ 5.515625 15.328125 5.515625 27.984375 \nQ 5.515625 40.671875 11.671875 48.328125 \nQ 17.828125 56 27.875 56 \nQ 33.9375 56 38.28125 53.609375 \nQ 42.625 51.21875 45.40625 46.390625 \nL 45.40625 54.6875 \nL 54.390625 54.6875 \nz\n\" id=\"DejaVuSans-103\"/>\n     </defs>\n     <use xlink:href=\"#DejaVuSans-80\"/>\n     <use x=\"56.677734\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"117.859375\" xlink:href=\"#DejaVuSans-108\"/>\n     <use x=\"145.642578\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"173.425781\" xlink:href=\"#DejaVuSans-99\"/>\n     <use x=\"228.40625\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"289.929688\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"321.716797\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"349.5\" xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"388.708984\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"450.232422\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"491.345703\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"552.625\" xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"591.833984\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"619.617188\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"680.798828\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"744.177734\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"775.964844\" xlink:href=\"#DejaVuSans-112\"/>\n     <use x=\"839.441406\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"900.964844\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"942.078125\" xlink:href=\"#DejaVuSans-102\"/>\n     <use x=\"977.283203\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"1038.464844\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"1077.828125\" xlink:href=\"#DejaVuSans-109\"/>\n     <use x=\"1175.240234\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"1236.519531\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"1299.898438\" xlink:href=\"#DejaVuSans-99\"/>\n     <use x=\"1354.878906\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"1416.402344\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"1448.189453\" xlink:href=\"#DejaVuSans-118\"/>\n     <use x=\"1507.369141\" xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"1559.46875\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"1591.255859\" xlink:href=\"#DejaVuSans-103\"/>\n     <use x=\"1654.732422\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"1716.011719\" xlink:href=\"#DejaVuSans-109\"/>\n     <use x=\"1813.423828\" xlink:href=\"#DejaVuSans-109\"/>\n     <use x=\"1910.835938\" xlink:href=\"#DejaVuSans-97\"/>\n    </g>\n   </g>\n  </g>\n  <g id=\"axes_2\">\n   <g id=\"patch_7\">\n    <path d=\"M 470.38125 241.718125 \nL 849.30625 241.718125 \nL 849.30625 22.318125 \nL 470.38125 22.318125 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_3\">\n    <g id=\"xtick_7\">\n     <g id=\"line2d_24\">\n      <path clip-path=\"url(#pc6f4858e70)\" d=\"M 487.605114 241.718125 \nL 487.605114 22.318125 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_25\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"487.605114\" xlink:href=\"#ma74f7f3276\" y=\"241.718125\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 0.0 -->\n      <g transform=\"translate(479.653551 256.316563)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_26\">\n      <path clip-path=\"url(#pc6f4858e70)\" d=\"M 556.500568 241.718125 \nL 556.500568 22.318125 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_27\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"556.500568\" xlink:href=\"#ma74f7f3276\" y=\"241.718125\"/>\n      </g>\n     </g>\n     <g id=\"text_16\">\n      <!-- 0.2 -->\n      <g transform=\"translate(548.549006 256.316563)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_9\">\n     <g id=\"line2d_28\">\n      <path clip-path=\"url(#pc6f4858e70)\" d=\"M 625.396023 241.718125 \nL 625.396023 22.318125 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_29\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"625.396023\" xlink:href=\"#ma74f7f3276\" y=\"241.718125\"/>\n      </g>\n     </g>\n     <g id=\"text_17\">\n      <!-- 0.4 -->\n      <g transform=\"translate(617.44446 256.316563)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_10\">\n     <g id=\"line2d_30\">\n      <path clip-path=\"url(#pc6f4858e70)\" d=\"M 694.291477 241.718125 \nL 694.291477 22.318125 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_31\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"694.291477\" xlink:href=\"#ma74f7f3276\" y=\"241.718125\"/>\n      </g>\n     </g>\n     <g id=\"text_18\">\n      <!-- 0.6 -->\n      <g transform=\"translate(686.339915 256.316563)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_11\">\n     <g id=\"line2d_32\">\n      <path clip-path=\"url(#pc6f4858e70)\" d=\"M 763.186932 241.718125 \nL 763.186932 22.318125 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_33\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"763.186932\" xlink:href=\"#ma74f7f3276\" y=\"241.718125\"/>\n      </g>\n     </g>\n     <g id=\"text_19\">\n      <!-- 0.8 -->\n      <g transform=\"translate(755.235369 256.316563)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_12\">\n     <g id=\"line2d_34\">\n      <path clip-path=\"url(#pc6f4858e70)\" d=\"M 832.082386 241.718125 \nL 832.082386 22.318125 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_35\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"832.082386\" xlink:href=\"#ma74f7f3276\" y=\"241.718125\"/>\n      </g>\n     </g>\n     <g id=\"text_20\">\n      <!-- 1.0 -->\n      <g transform=\"translate(824.130824 256.316563)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_21\">\n     <!-- Gamma -->\n     <g transform=\"translate(640.100781 269.994688)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-71\"/>\n      <use x=\"77.490234\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"138.769531\" xlink:href=\"#DejaVuSans-109\"/>\n      <use x=\"236.181641\" xlink:href=\"#DejaVuSans-109\"/>\n      <use x=\"333.59375\" xlink:href=\"#DejaVuSans-97\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_4\">\n    <g id=\"ytick_6\">\n     <g id=\"line2d_36\">\n      <path clip-path=\"url(#pc6f4858e70)\" d=\"M 470.38125 231.745398 \nL 849.30625 231.745398 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_37\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"470.38125\" xlink:href=\"#m65960bc0ca\" y=\"231.745398\"/>\n      </g>\n     </g>\n     <g id=\"text_22\">\n      <!-- 0.0 -->\n      <g transform=\"translate(447.478125 235.544616)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_38\">\n      <path clip-path=\"url(#pc6f4858e70)\" d=\"M 470.38125 185.735122 \nL 849.30625 185.735122 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_39\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"470.38125\" xlink:href=\"#m65960bc0ca\" y=\"185.735122\"/>\n      </g>\n     </g>\n     <g id=\"text_23\">\n      <!-- 0.2 -->\n      <g transform=\"translate(447.478125 189.534341)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_40\">\n      <path clip-path=\"url(#pc6f4858e70)\" d=\"M 470.38125 139.724846 \nL 849.30625 139.724846 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_41\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"470.38125\" xlink:href=\"#m65960bc0ca\" y=\"139.724846\"/>\n      </g>\n     </g>\n     <g id=\"text_24\">\n      <!-- 0.4 -->\n      <g transform=\"translate(447.478125 143.524065)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_9\">\n     <g id=\"line2d_42\">\n      <path clip-path=\"url(#pc6f4858e70)\" d=\"M 470.38125 93.71457 \nL 849.30625 93.71457 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_43\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"470.38125\" xlink:href=\"#m65960bc0ca\" y=\"93.71457\"/>\n      </g>\n     </g>\n     <g id=\"text_25\">\n      <!-- 0.6 -->\n      <g transform=\"translate(447.478125 97.513789)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_10\">\n     <g id=\"line2d_44\">\n      <path clip-path=\"url(#pc6f4858e70)\" d=\"M 470.38125 47.704295 \nL 849.30625 47.704295 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_45\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"470.38125\" xlink:href=\"#m65960bc0ca\" y=\"47.704295\"/>\n      </g>\n     </g>\n     <g id=\"text_26\">\n      <!-- 0.8 -->\n      <g transform=\"translate(447.478125 51.503513)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_27\">\n     <!-- Mean rewards -->\n     <g transform=\"translate(441.398438 167.150156)rotate(-90)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-77\"/>\n      <use x=\"86.279297\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"147.802734\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"209.082031\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"272.460938\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"304.248047\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"343.111328\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"404.634766\" xlink:href=\"#DejaVuSans-119\"/>\n      <use x=\"486.421875\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"547.701172\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"587.064453\" xlink:href=\"#DejaVuSans-100\"/>\n      <use x=\"650.541016\" xlink:href=\"#DejaVuSans-115\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_46\">\n    <path clip-path=\"url(#pc6f4858e70)\" d=\"M 487.605114 231.745398 \nL 522.052841 115.109349 \nL 556.500568 104.987088 \nL 590.948295 104.987088 \nL 625.396023 104.987088 \nL 659.84375 104.987088 \nL 694.291477 104.987088 \nL 728.739205 94.174673 \nL 763.186932 83.362258 \nL 797.634659 66.568508 \nL 832.082386 32.290852 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_8\">\n    <path d=\"M 470.38125 241.718125 \nL 470.38125 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_9\">\n    <path d=\"M 849.30625 241.718125 \nL 849.30625 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_10\">\n    <path d=\"M 470.38125 241.718125 \nL 849.30625 241.718125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_11\">\n    <path d=\"M 470.38125 22.318125 \nL 849.30625 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_28\">\n    <!-- Value iteration performance vs gamma -->\n    <g transform=\"translate(542.435938 16.318125)scale(0.12 -0.12)\">\n     <defs>\n      <path d=\"M 28.609375 0 \nL 0.78125 72.90625 \nL 11.078125 72.90625 \nL 34.1875 11.53125 \nL 57.328125 72.90625 \nL 67.578125 72.90625 \nL 39.796875 0 \nz\n\" id=\"DejaVuSans-86\"/>\n      <path d=\"M 8.5 21.578125 \nL 8.5 54.6875 \nL 17.484375 54.6875 \nL 17.484375 21.921875 \nQ 17.484375 14.15625 20.5 10.265625 \nQ 23.53125 6.390625 29.59375 6.390625 \nQ 36.859375 6.390625 41.078125 11.03125 \nQ 45.3125 15.671875 45.3125 23.6875 \nL 45.3125 54.6875 \nL 54.296875 54.6875 \nL 54.296875 0 \nL 45.3125 0 \nL 45.3125 8.40625 \nQ 42.046875 3.421875 37.71875 1 \nQ 33.40625 -1.421875 27.6875 -1.421875 \nQ 18.265625 -1.421875 13.375 4.4375 \nQ 8.5 10.296875 8.5 21.578125 \nz\nM 31.109375 56 \nz\n\" id=\"DejaVuSans-117\"/>\n     </defs>\n     <use xlink:href=\"#DejaVuSans-86\"/>\n     <use x=\"60.658203\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"121.9375\" xlink:href=\"#DejaVuSans-108\"/>\n     <use x=\"149.720703\" xlink:href=\"#DejaVuSans-117\"/>\n     <use x=\"213.099609\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"274.623047\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"306.410156\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"334.193359\" xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"373.402344\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"434.925781\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"476.039062\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"537.318359\" xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"576.527344\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"604.310547\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"665.492188\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"728.871094\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"760.658203\" xlink:href=\"#DejaVuSans-112\"/>\n     <use x=\"824.134766\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"885.658203\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"926.771484\" xlink:href=\"#DejaVuSans-102\"/>\n     <use x=\"961.976562\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"1023.158203\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"1062.521484\" xlink:href=\"#DejaVuSans-109\"/>\n     <use x=\"1159.933594\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"1221.212891\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"1284.591797\" xlink:href=\"#DejaVuSans-99\"/>\n     <use x=\"1339.572266\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"1401.095703\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"1432.882812\" xlink:href=\"#DejaVuSans-118\"/>\n     <use x=\"1492.0625\" xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"1544.162109\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"1575.949219\" xlink:href=\"#DejaVuSans-103\"/>\n     <use x=\"1639.425781\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"1700.705078\" xlink:href=\"#DejaVuSans-109\"/>\n     <use x=\"1798.117188\" xlink:href=\"#DejaVuSans-109\"/>\n     <use x=\"1895.529297\" xlink:href=\"#DejaVuSans-97\"/>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p3281c39a5f\">\n   <rect height=\"219.4\" width=\"378.925\" x=\"43.78125\" y=\"22.318125\"/>\n  </clipPath>\n  <clipPath id=\"pc6f4858e70\">\n   <rect height=\"219.4\" width=\"378.925\" x=\"470.38125\" y=\"22.318125\"/>\n  </clipPath>\n </defs>\n</svg>\n","image/png":"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\n"},"metadata":{"needs_background":"light"}}],"source":["import matplotlib.pyplot as plt\n","fig, axes = plt.subplots(1,2, figsize = (12,4))\n","x_axis = np.arange(0,1.1,0.1)\n","# x_axis = np.arange(0,1.0,0.1)\n","axes[0].plot(x_axis, rewards_pi)\n","axes[0].grid(True)\n","axes[0].set_xlabel(\"Gamma\")\n","axes[0].set_ylabel(\"Mean rewards\")\n","axes[0].set_title('Police iteration performance vs gamma')\n","axes[1].plot(x_axis, rewards_vi)\n","axes[1].grid(True)\n","axes[1].set_xlabel(\"Gamma\")\n","axes[1].set_ylabel(\"Mean rewards\")\n","axes[1].set_title('Value iteration performance vs gamma')\n","fig.tight_layout()"]},{"cell_type":"markdown","metadata":{},"source":[" 3. What is the impact of the value function convergence criterion epsilon?\n"," \n","    As shown in the result the $\\epsilon$ can froces the model learn more accurate value function,\n","    which will reduce the randomness of the model performance but won't influence model final performance much.\n","    And it will only affect the policy iteration method.\n","\n","    However if epsilon equal or larger than the 1.0 the model may not convergence.\n"]},{"cell_type":"code","execution_count":23,"metadata":{"tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":"current epsilon:0.1\n[INFO]\tIn 0 iteration, current mean episode reward is 0.0.\n[INFO]\tIn 1 iteration, current mean episode reward is 0.0.\n[INFO]\tIn 2 iteration, current mean episode reward is 0.0.\n[INFO]\tIn 3 iteration, current mean episode reward is 0.0.\n[INFO]\tIn 4 iteration, current mean episode reward is 0.0.\n[INFO]\tIn 5 iteration, current mean episode reward is 0.542.\n[INFO]\tIn 6 iteration, current mean episode reward is 0.868.\n[INFO]\tIn 7 iteration, current mean episode reward is 0.885.\n[INFO]\tIn 8 iteration, current mean episode reward is 0.885.\nWe found policy is not changed anymore at itertaion 9. Current mean episode reward is 0.885. Stop training.\ncurrent epsilon:0.01\n[INFO]\tIn 0 iteration, current mean episode reward is 0.065.\n[INFO]\tIn 1 iteration, current mean episode reward is 0.58.\n[INFO]\tIn 2 iteration, current mean episode reward is 0.806.\n[DEBUG]\tUpdated values for 200 steps. Difference between new and old table is: 0.02277778414538628\n[INFO]\tIn 3 iteration, current mean episode reward is 0.865.\n[INFO]\tIn 4 iteration, current mean episode reward is 0.854.\n[INFO]\tIn 5 iteration, current mean episode reward is 0.867.\n[INFO]\tIn 6 iteration, current mean episode reward is 0.867.\nWe found policy is not changed anymore at itertaion 7. Current mean episode reward is 0.867. Stop training.\ncurrent epsilon:0.001\n[INFO]\tIn 0 iteration, current mean episode reward is 0.418.\n[DEBUG]\tUpdated values for 200 steps. Difference between new and old table is: 0.004818008156197144\n[INFO]\tIn 1 iteration, current mean episode reward is 0.864.\n[DEBUG]\tUpdated values for 200 steps. Difference between new and old table is: 0.01826634480612055\n[INFO]\tIn 2 iteration, current mean episode reward is 0.77.\n[INFO]\tIn 3 iteration, current mean episode reward is 0.8.\n[INFO]\tIn 4 iteration, current mean episode reward is 0.867.\nWe found policy is not changed anymore at itertaion 5. Current mean episode reward is 0.867. Stop training.\ncurrent epsilon:0.0001\n[INFO]\tIn 0 iteration, current mean episode reward is 0.019.\n[DEBUG]\tUpdated values for 200 steps. Difference between new and old table is: 0.00011145618073658695\n[INFO]\tIn 1 iteration, current mean episode reward is 0.312.\n[DEBUG]\tUpdated values for 200 steps. Difference between new and old table is: 0.04850714131006392\n[DEBUG]\tUpdated values for 400 steps. Difference between new and old table is: 0.008118342720085159\n[DEBUG]\tUpdated values for 600 steps. Difference between new and old table is: 0.0011306660548398156\n[DEBUG]\tUpdated values for 800 steps. Difference between new and old table is: 0.00014831307869965504\n[INFO]\tIn 2 iteration, current mean episode reward is 0.82.\n[DEBUG]\tUpdated values for 200 steps. Difference between new and old table is: 0.0010509127305443206\n[INFO]\tIn 3 iteration, current mean episode reward is 0.805.\n[DEBUG]\tUpdated values for 200 steps. Difference between new and old table is: 0.0009078138780765493\n[INFO]\tIn 4 iteration, current mean episode reward is 0.77.\n[DEBUG]\tUpdated values for 200 steps. Difference between new and old table is: 0.0004625542572498642\n[INFO]\tIn 5 iteration, current mean episode reward is 0.688.\n[DEBUG]\tUpdated values for 200 steps. Difference between new and old table is: 0.00016807637843632706\n[INFO]\tIn 6 iteration, current mean episode reward is 0.87.\n[INFO]\tIn 7 iteration, current mean episode reward is 0.867.\nWe found policy is not changed anymore at itertaion 8. Current mean episode reward is 0.867. Stop training.\ncurrent epsilon:1e-05\n[INFO]\tIn 0 iteration, current mean episode reward is 0.068.\n[DEBUG]\tUpdated values for 200 steps. Difference between new and old table is: 0.0069023098018124885\n[DEBUG]\tUpdated values for 400 steps. Difference between new and old table is: 0.0013855947605159059\n[DEBUG]\tUpdated values for 600 steps. Difference between new and old table is: 0.00027809812443613594\n[DEBUG]\tUpdated values for 800 steps. Difference between new and old table is: 5.581615079827337e-05\n[DEBUG]\tUpdated values for 1000 steps. Difference between new and old table is: 1.1202674222462669e-05\n[INFO]\tIn 1 iteration, current mean episode reward is 0.589.\n[DEBUG]\tUpdated values for 200 steps. Difference between new and old table is: 0.014499174984301894\n[DEBUG]\tUpdated values for 400 steps. Difference between new and old table is: 0.00026964712636774746\n[INFO]\tIn 2 iteration, current mean episode reward is 0.77.\n[DEBUG]\tUpdated values for 200 steps. Difference between new and old table is: 0.02959376210464304\n[DEBUG]\tUpdated values for 400 steps. Difference between new and old table is: 0.0024603891254183358\n[DEBUG]\tUpdated values for 600 steps. Difference between new and old table is: 0.0001503232407577687\n[INFO]\tIn 3 iteration, current mean episode reward is 0.688.\n[DEBUG]\tUpdated values for 200 steps. Difference between new and old table is: 0.00022748737427272236\n[DEBUG]\tUpdated values for 400 steps. Difference between new and old table is: 1.2407712608270005e-05\n[INFO]\tIn 4 iteration, current mean episode reward is 0.87.\n[INFO]\tIn 5 iteration, current mean episode reward is 0.867.\nWe found policy is not changed anymore at itertaion 6. Current mean episode reward is 0.867. Stop training.\ncurrent epsilon:1e-06\n[INFO]\tIn 0 iteration, current mean episode reward is 0.127.\n[DEBUG]\tUpdated values for 200 steps. Difference between new and old table is: 0.02910256615083348\n[DEBUG]\tUpdated values for 400 steps. Difference between new and old table is: 0.006209905966121476\n[DEBUG]\tUpdated values for 600 steps. Difference between new and old table is: 0.0020121316061079905\n[DEBUG]\tUpdated values for 800 steps. Difference between new and old table is: 0.0007136716883745335\n[DEBUG]\tUpdated values for 1000 steps. Difference between new and old table is: 0.0002567772402805382\n[DEBUG]\tUpdated values for 1200 steps. Difference between new and old table is: 9.258493734781367e-05\n[DEBUG]\tUpdated values for 1400 steps. Difference between new and old table is: 3.3393406410790055e-05\n[DEBUG]\tUpdated values for 1600 steps. Difference between new and old table is: 1.2044845393362202e-05\n[DEBUG]\tUpdated values for 1800 steps. Difference between new and old table is: 4.344549004092063e-06\n[DEBUG]\tUpdated values for 2000 steps. Difference between new and old table is: 1.567070759753153e-06\n[INFO]\tIn 1 iteration, current mean episode reward is 0.602.\n[DEBUG]\tUpdated values for 200 steps. Difference between new and old table is: 6.684695210583402e-05\n[INFO]\tIn 2 iteration, current mean episode reward is 0.877.\n[DEBUG]\tUpdated values for 200 steps. Difference between new and old table is: 1.264280248995664e-05\n[INFO]\tIn 3 iteration, current mean episode reward is 0.688.\n[DEBUG]\tUpdated values for 200 steps. Difference between new and old table is: 0.00017624770822534386\n[DEBUG]\tUpdated values for 400 steps. Difference between new and old table is: 8.939071335381521e-06\n[INFO]\tIn 4 iteration, current mean episode reward is 0.87.\n[DEBUG]\tUpdated values for 200 steps. Difference between new and old table is: 4.637087048844912e-05\n[INFO]\tIn 5 iteration, current mean episode reward is 0.867.\n[DEBUG]\tUpdated values for 200 steps. Difference between new and old table is: 6.186900529836503e-05\n[DEBUG]\tUpdated values for 400 steps. Difference between new and old table is: 1.2368591993938693e-06\n[INFO]\tIn 6 iteration, current mean episode reward is 0.863.\n[INFO]\tIn 7 iteration, current mean episode reward is 0.844.\n[INFO]\tIn 8 iteration, current mean episode reward is 0.854.\nWe found policy is not changed anymore at itertaion 9. Current mean episode reward is 0.854. Stop training.\ncurrent epsilon:1e-07\n[INFO]\tIn 0 iteration, current mean episode reward is 0.0.\n[INFO]\tIn 1 iteration, current mean episode reward is 0.0.\n[DEBUG]\tUpdated values for 200 steps. Difference between new and old table is: 0.006237723554672336\n[DEBUG]\tUpdated values for 400 steps. Difference between new and old table is: 0.00033138654439407444\n[DEBUG]\tUpdated values for 600 steps. Difference between new and old table is: 1.7605307584545038e-05\n[DEBUG]\tUpdated values for 800 steps. Difference between new and old table is: 9.353030782088823e-07\n[INFO]\tIn 2 iteration, current mean episode reward is 0.0.\n[INFO]\tIn 3 iteration, current mean episode reward is 0.0.\n[INFO]\tIn 4 iteration, current mean episode reward is 0.0.\n[INFO]\tIn 5 iteration, current mean episode reward is 0.0.\n[INFO]\tIn 6 iteration, current mean episode reward is 0.538.\n[DEBUG]\tUpdated values for 200 steps. Difference between new and old table is: 4.0749818018201434e-07\n[INFO]\tIn 7 iteration, current mean episode reward is 0.867.\n[DEBUG]\tUpdated values for 200 steps. Difference between new and old table is: 1.461952607796635e-05\n[INFO]\tIn 8 iteration, current mean episode reward is 0.688.\n[DEBUG]\tUpdated values for 200 steps. Difference between new and old table is: 0.00015285193159716992\n[DEBUG]\tUpdated values for 400 steps. Difference between new and old table is: 8.113675031173884e-06\n[DEBUG]\tUpdated values for 600 steps. Difference between new and old table is: 4.3104870856014443e-07\n[INFO]\tIn 9 iteration, current mean episode reward is 0.87.\n[DEBUG]\tUpdated values for 200 steps. Difference between new and old table is: 7.792143604398727e-06\n[INFO]\tIn 10 iteration, current mean episode reward is 0.867.\n[DEBUG]\tUpdated values for 200 steps. Difference between new and old table is: 4.6363663498291685e-05\n[DEBUG]\tUpdated values for 400 steps. Difference between new and old table is: 2.1261603172506582e-07\n[INFO]\tIn 11 iteration, current mean episode reward is 0.863.\n[DEBUG]\tUpdated values for 200 steps. Difference between new and old table is: 6.18546442222867e-05\n[DEBUG]\tUpdated values for 400 steps. Difference between new and old table is: 1.2365720992862927e-06\n[INFO]\tIn 12 iteration, current mean episode reward is 0.854.\nWe found policy is not changed anymore at itertaion 13. Current mean episode reward is 0.854. Stop training.\ncurrent epsilon:1e-08\n[INFO]\tIn 0 iteration, current mean episode reward is 0.0.\n[INFO]\tIn 1 iteration, current mean episode reward is 0.0.\n[DEBUG]\tUpdated values for 200 steps. Difference between new and old table is: 0.01018341866528463\n[DEBUG]\tUpdated values for 400 steps. Difference between new and old table is: 0.0005410063289972749\n[DEBUG]\tUpdated values for 600 steps. Difference between new and old table is: 2.874161002751948e-05\n[DEBUG]\tUpdated values for 800 steps. Difference between new and old table is: 1.5269325008230444e-06\n[DEBUG]\tUpdated values for 1000 steps. Difference between new and old table is: 8.1120120471595e-08\n[INFO]\tIn 2 iteration, current mean episode reward is 0.0.\n[INFO]\tIn 3 iteration, current mean episode reward is 0.0.\n[INFO]\tIn 4 iteration, current mean episode reward is 0.0.\n[INFO]\tIn 5 iteration, current mean episode reward is 0.538.\n[DEBUG]\tUpdated values for 200 steps. Difference between new and old table is: 4.073908717455721e-07\n[INFO]\tIn 6 iteration, current mean episode reward is 0.867.\n[DEBUG]\tUpdated values for 200 steps. Difference between new and old table is: 1.4619519035433126e-05\n[INFO]\tIn 7 iteration, current mean episode reward is 0.688.\n[DEBUG]\tUpdated values for 200 steps. Difference between new and old table is: 0.0001528510988770826\n[DEBUG]\tUpdated values for 400 steps. Difference between new and old table is: 8.113630786565906e-06\n[DEBUG]\tUpdated values for 600 steps. Difference between new and old table is: 4.3104635748247855e-07\n[DEBUG]\tUpdated values for 800 steps. Difference between new and old table is: 2.2899854706937006e-08\n[INFO]\tIn 8 iteration, current mean episode reward is 0.87.\n[DEBUG]\tUpdated values for 200 steps. Difference between new and old table is: 7.792014103918943e-06\n[INFO]\tIn 9 iteration, current mean episode reward is 0.856.\n[DEBUG]\tUpdated values for 200 steps. Difference between new and old table is: 4.6363365433205184e-05\n[DEBUG]\tUpdated values for 400 steps. Difference between new and old table is: 2.126146641384663e-07\n[INFO]\tIn 10 iteration, current mean episode reward is 0.845.\n[DEBUG]\tUpdated values for 200 steps. Difference between new and old table is: 6.185323096107032e-05\n[DEBUG]\tUpdated values for 400 steps. Difference between new and old table is: 1.2365438456507505e-06\n[DEBUG]\tUpdated values for 600 steps. Difference between new and old table is: 2.47204653969213e-08\n[INFO]\tIn 11 iteration, current mean episode reward is 0.863.\n[INFO]\tIn 12 iteration, current mean episode reward is 0.844.\n[INFO]\tIn 13 iteration, current mean episode reward is 0.854.\nWe found policy is not changed anymore at itertaion 14. Current mean episode reward is 0.854. Stop training.\ncurrent epsilon:1e-09\n[DEBUG]\tUpdated values for 200 steps. Difference between new and old table is: 3.0181359723897914e-05\n[DEBUG]\tUpdated values for 400 steps. Difference between new and old table is: 2.187875441064704e-08\n[INFO]\tIn 0 iteration, current mean episode reward is 0.559.\n[DEBUG]\tUpdated values for 200 steps. Difference between new and old table is: 0.007883944673792484\n[DEBUG]\tUpdated values for 400 steps. Difference between new and old table is: 7.973990334196601e-05\n[DEBUG]\tUpdated values for 600 steps. Difference between new and old table is: 7.962620071216842e-07\n[DEBUG]\tUpdated values for 800 steps. Difference between new and old table is: 7.943277605115817e-09\n[INFO]\tIn 1 iteration, current mean episode reward is 0.851.\n[DEBUG]\tUpdated values for 200 steps. Difference between new and old table is: 0.024525240665375245\n[DEBUG]\tUpdated values for 400 steps. Difference between new and old table is: 0.001693921181638372\n[DEBUG]\tUpdated values for 600 steps. Difference between new and old table is: 9.802607598205504e-05\n[DEBUG]\tUpdated values for 800 steps. Difference between new and old table is: 5.373135662886663e-06\n[DEBUG]\tUpdated values for 1000 steps. Difference between new and old table is: 2.8885859597316266e-07\n[DEBUG]\tUpdated values for 1200 steps. Difference between new and old table is: 1.5416035031856623e-08\n[INFO]\tIn 2 iteration, current mean episode reward is 0.77.\n[DEBUG]\tUpdated values for 200 steps. Difference between new and old table is: 0.0005052357630222215\n[DEBUG]\tUpdated values for 400 steps. Difference between new and old table is: 3.316798145122646e-05\n[DEBUG]\tUpdated values for 600 steps. Difference between new and old table is: 1.8961521881244447e-06\n[DEBUG]\tUpdated values for 800 steps. Difference between new and old table is: 1.0383553644688348e-07\n[DEBUG]\tUpdated values for 1000 steps. Difference between new and old table is: 5.601115599107587e-09\n[INFO]\tIn 3 iteration, current mean episode reward is 0.688.\n[DEBUG]\tUpdated values for 200 steps. Difference between new and old table is: 0.0001911298796691968\n[DEBUG]\tUpdated values for 400 steps. Difference between new and old table is: 1.8000332596621038e-05\n[DEBUG]\tUpdated values for 600 steps. Difference between new and old table is: 1.3731348682544109e-06\n[DEBUG]\tUpdated values for 800 steps. Difference between new and old table is: 9.509485965342179e-08\n[DEBUG]\tUpdated values for 1000 steps. Difference between new and old table is: 6.228530580187908e-09\n[INFO]\tIn 4 iteration, current mean episode reward is 0.829.\n[INFO]\tIn 5 iteration, current mean episode reward is 0.867.\nWe found policy is not changed anymore at itertaion 6. Current mean episode reward is 0.867. Stop training.\n"},{"output_type":"error","ename":"IndexError","evalue":"list index out of range","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-23-b7aafee8ecea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcurr_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefault_pi_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mcurr_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'current epsilon:{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mpi_agent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: list index out of range"]}],"source":["rewards_pi = []\n","import math\n","eps = [1/10.0**i for i in np.arange(1,10,1)]\n","for i in range(10):\n","    curr_config = default_pi_config.copy()\n","    curr_config['eps'] = eps[i]\n","    print('current epsilon:{}'.format(eps[i]))\n","    pi_agent = policy_iteration(curr_config)\n","    rewards_pi.append(pi_agent.evaluate())\n"]},{"cell_type":"code","execution_count":24,"metadata":{"tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":"[0.1, 0.01, 0.001, 0.0001, 1e-05, 1e-06, 1e-07, 1e-08, 1e-09]\n"},{"output_type":"execute_result","data":{"text/plain":"Text(0.5, 1.0, 'Police iteration performance vs epsilon')"},"metadata":{},"execution_count":24},{"output_type":"display_data","data":{"text/plain":"<Figure size 864x288 with 1 Axes>","image/svg+xml":"<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"277.314375pt\" version=\"1.1\" viewBox=\"0 0 733.30625 277.314375\" width=\"733.30625pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2020-09-23T19:46:27.435047</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.3.2, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 277.314375 \nL 733.30625 277.314375 \nL 733.30625 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 56.50625 239.758125 \nL 726.10625 239.758125 \nL 726.10625 22.318125 \nL 56.50625 22.318125 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path clip-path=\"url(#pf7b7d9e3a1)\" d=\"M 695.669886 239.758125 \nL 695.669886 22.318125 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_2\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m0ce4b9702a\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"695.669886\" xlink:href=\"#m0ce4b9702a\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0.1 -->\n      <g transform=\"translate(687.718324 254.356562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n        <path d=\"M 10.6875 12.40625 \nL 21 12.40625 \nL 21 0 \nL 10.6875 0 \nz\n\" id=\"DejaVuSans-46\"/>\n        <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-49\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_3\">\n      <path clip-path=\"url(#pf7b7d9e3a1)\" d=\"M 619.578977 239.758125 \nL 619.578977 22.318125 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"619.578977\" xlink:href=\"#m0ce4b9702a\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 0.01 -->\n      <g transform=\"translate(608.446165 254.356562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-49\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_5\">\n      <path clip-path=\"url(#pf7b7d9e3a1)\" d=\"M 543.488068 239.758125 \nL 543.488068 22.318125 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"543.488068\" xlink:href=\"#m0ce4b9702a\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 0.001 -->\n      <g transform=\"translate(529.174006 254.356562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-49\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_7\">\n      <path clip-path=\"url(#pf7b7d9e3a1)\" d=\"M 467.397159 239.758125 \nL 467.397159 22.318125 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"467.397159\" xlink:href=\"#m0ce4b9702a\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 0.0001 -->\n      <g transform=\"translate(449.901847 254.356562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"286.279297\" xlink:href=\"#DejaVuSans-49\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_9\">\n      <path clip-path=\"url(#pf7b7d9e3a1)\" d=\"M 391.30625 239.758125 \nL 391.30625 22.318125 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"391.30625\" xlink:href=\"#m0ce4b9702a\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 1e-05 -->\n      <g transform=\"translate(376.882031 254.356562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n        <path d=\"M 4.890625 31.390625 \nL 31.203125 31.390625 \nL 31.203125 23.390625 \nL 4.890625 23.390625 \nz\n\" id=\"DejaVuSans-45\"/>\n        <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-101\"/>\n       <use x=\"125.146484\" xlink:href=\"#DejaVuSans-45\"/>\n       <use x=\"161.230469\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"224.853516\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_11\">\n      <path clip-path=\"url(#pf7b7d9e3a1)\" d=\"M 315.215341 239.758125 \nL 315.215341 22.318125 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"315.215341\" xlink:href=\"#m0ce4b9702a\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 1e-06 -->\n      <g transform=\"translate(300.791122 254.356562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-101\"/>\n       <use x=\"125.146484\" xlink:href=\"#DejaVuSans-45\"/>\n       <use x=\"161.230469\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"224.853516\" xlink:href=\"#DejaVuSans-54\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_13\">\n      <path clip-path=\"url(#pf7b7d9e3a1)\" d=\"M 239.124432 239.758125 \nL 239.124432 22.318125 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"239.124432\" xlink:href=\"#m0ce4b9702a\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 1e-07 -->\n      <g transform=\"translate(224.700213 254.356562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 8.203125 72.90625 \nL 55.078125 72.90625 \nL 55.078125 68.703125 \nL 28.609375 0 \nL 18.3125 0 \nL 43.21875 64.59375 \nL 8.203125 64.59375 \nz\n\" id=\"DejaVuSans-55\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-101\"/>\n       <use x=\"125.146484\" xlink:href=\"#DejaVuSans-45\"/>\n       <use x=\"161.230469\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"224.853516\" xlink:href=\"#DejaVuSans-55\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_15\">\n      <path clip-path=\"url(#pf7b7d9e3a1)\" d=\"M 163.033523 239.758125 \nL 163.033523 22.318125 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"163.033523\" xlink:href=\"#m0ce4b9702a\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 1e-08 -->\n      <g transform=\"translate(148.609304 254.356562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-101\"/>\n       <use x=\"125.146484\" xlink:href=\"#DejaVuSans-45\"/>\n       <use x=\"161.230469\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"224.853516\" xlink:href=\"#DejaVuSans-56\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_9\">\n     <g id=\"line2d_17\">\n      <path clip-path=\"url(#pf7b7d9e3a1)\" d=\"M 86.942614 239.758125 \nL 86.942614 22.318125 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_18\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"86.942614\" xlink:href=\"#m0ce4b9702a\" y=\"239.758125\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 1e-09 -->\n      <g transform=\"translate(72.518395 254.356562)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 10.984375 1.515625 \nL 10.984375 10.5 \nQ 14.703125 8.734375 18.5 7.8125 \nQ 22.3125 6.890625 25.984375 6.890625 \nQ 35.75 6.890625 40.890625 13.453125 \nQ 46.046875 20.015625 46.78125 33.40625 \nQ 43.953125 29.203125 39.59375 26.953125 \nQ 35.25 24.703125 29.984375 24.703125 \nQ 19.046875 24.703125 12.671875 31.3125 \nQ 6.296875 37.9375 6.296875 49.421875 \nQ 6.296875 60.640625 12.9375 67.421875 \nQ 19.578125 74.21875 30.609375 74.21875 \nQ 43.265625 74.21875 49.921875 64.515625 \nQ 56.59375 54.828125 56.59375 36.375 \nQ 56.59375 19.140625 48.40625 8.859375 \nQ 40.234375 -1.421875 26.421875 -1.421875 \nQ 22.703125 -1.421875 18.890625 -0.6875 \nQ 15.09375 0.046875 10.984375 1.515625 \nz\nM 30.609375 32.421875 \nQ 37.25 32.421875 41.125 36.953125 \nQ 45.015625 41.5 45.015625 49.421875 \nQ 45.015625 57.28125 41.125 61.84375 \nQ 37.25 66.40625 30.609375 66.40625 \nQ 23.96875 66.40625 20.09375 61.84375 \nQ 16.21875 57.28125 16.21875 49.421875 \nQ 16.21875 41.5 20.09375 36.953125 \nQ 23.96875 32.421875 30.609375 32.421875 \nz\n\" id=\"DejaVuSans-57\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-101\"/>\n       <use x=\"125.146484\" xlink:href=\"#DejaVuSans-45\"/>\n       <use x=\"161.230469\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"224.853516\" xlink:href=\"#DejaVuSans-57\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_10\">\n     <!-- Epsilon -->\n     <g transform=\"translate(373.361719 268.034687)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 9.8125 72.90625 \nL 55.90625 72.90625 \nL 55.90625 64.59375 \nL 19.671875 64.59375 \nL 19.671875 43.015625 \nL 54.390625 43.015625 \nL 54.390625 34.71875 \nL 19.671875 34.71875 \nL 19.671875 8.296875 \nL 56.78125 8.296875 \nL 56.78125 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-69\"/>\n       <path d=\"M 18.109375 8.203125 \nL 18.109375 -20.796875 \nL 9.078125 -20.796875 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nz\nM 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\n\" id=\"DejaVuSans-112\"/>\n       <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n       <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n       <path d=\"M 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 0 \nL 9.421875 0 \nz\n\" id=\"DejaVuSans-108\"/>\n       <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n       <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"63.183594\" xlink:href=\"#DejaVuSans-112\"/>\n      <use x=\"126.660156\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"178.759766\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"206.542969\" xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"234.326172\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"295.507812\" xlink:href=\"#DejaVuSans-110\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_19\">\n      <path clip-path=\"url(#pf7b7d9e3a1)\" d=\"M 56.50625 223.497949 \nL 726.10625 223.497949 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_20\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m16990d0085\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"56.50625\" xlink:href=\"#m16990d0085\" y=\"223.497949\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 0.855 -->\n      <g transform=\"translate(20.878125 227.297168)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_21\">\n      <path clip-path=\"url(#pf7b7d9e3a1)\" d=\"M 56.50625 191.615251 \nL 726.10625 191.615251 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_22\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"56.50625\" xlink:href=\"#m16990d0085\" y=\"191.615251\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 0.860 -->\n      <g transform=\"translate(20.878125 195.41447)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_23\">\n      <path clip-path=\"url(#pf7b7d9e3a1)\" d=\"M 56.50625 159.732553 \nL 726.10625 159.732553 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_24\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"56.50625\" xlink:href=\"#m16990d0085\" y=\"159.732553\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 0.865 -->\n      <g transform=\"translate(20.878125 163.531772)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_25\">\n      <path clip-path=\"url(#pf7b7d9e3a1)\" d=\"M 56.50625 127.849855 \nL 726.10625 127.849855 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_26\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"56.50625\" xlink:href=\"#m16990d0085\" y=\"127.849855\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 0.870 -->\n      <g transform=\"translate(20.878125 131.649074)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_27\">\n      <path clip-path=\"url(#pf7b7d9e3a1)\" d=\"M 56.50625 95.967157 \nL 726.10625 95.967157 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_28\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"56.50625\" xlink:href=\"#m16990d0085\" y=\"95.967157\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 0.875 -->\n      <g transform=\"translate(20.878125 99.766376)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_29\">\n      <path clip-path=\"url(#pf7b7d9e3a1)\" d=\"M 56.50625 64.084459 \nL 726.10625 64.084459 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_30\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"56.50625\" xlink:href=\"#m16990d0085\" y=\"64.084459\"/>\n      </g>\n     </g>\n     <g id=\"text_16\">\n      <!-- 0.880 -->\n      <g transform=\"translate(20.878125 67.883678)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-56\"/>\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_31\">\n      <path clip-path=\"url(#pf7b7d9e3a1)\" d=\"M 56.50625 32.201761 \nL 726.10625 32.201761 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_32\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"56.50625\" xlink:href=\"#m16990d0085\" y=\"32.201761\"/>\n      </g>\n     </g>\n     <g id=\"text_17\">\n      <!-- 0.885 -->\n      <g transform=\"translate(20.878125 36.00098)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-56\"/>\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_18\">\n     <!-- Mean Rewards -->\n     <g transform=\"translate(14.798437 167.476406)rotate(-90)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 9.8125 72.90625 \nL 24.515625 72.90625 \nL 43.109375 23.296875 \nL 61.8125 72.90625 \nL 76.515625 72.90625 \nL 76.515625 0 \nL 66.890625 0 \nL 66.890625 64.015625 \nL 48.09375 14.015625 \nL 38.1875 14.015625 \nL 19.390625 64.015625 \nL 19.390625 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-77\"/>\n       <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n       <path id=\"DejaVuSans-32\"/>\n       <path d=\"M 44.390625 34.1875 \nQ 47.5625 33.109375 50.5625 29.59375 \nQ 53.5625 26.078125 56.59375 19.921875 \nL 66.609375 0 \nL 56 0 \nL 46.6875 18.703125 \nQ 43.0625 26.03125 39.671875 28.421875 \nQ 36.28125 30.8125 30.421875 30.8125 \nL 19.671875 30.8125 \nL 19.671875 0 \nL 9.8125 0 \nL 9.8125 72.90625 \nL 32.078125 72.90625 \nQ 44.578125 72.90625 50.734375 67.671875 \nQ 56.890625 62.453125 56.890625 51.90625 \nQ 56.890625 45.015625 53.6875 40.46875 \nQ 50.484375 35.9375 44.390625 34.1875 \nz\nM 19.671875 64.796875 \nL 19.671875 38.921875 \nL 32.078125 38.921875 \nQ 39.203125 38.921875 42.84375 42.21875 \nQ 46.484375 45.515625 46.484375 51.90625 \nQ 46.484375 58.296875 42.84375 61.546875 \nQ 39.203125 64.796875 32.078125 64.796875 \nz\n\" id=\"DejaVuSans-82\"/>\n       <path d=\"M 4.203125 54.6875 \nL 13.1875 54.6875 \nL 24.421875 12.015625 \nL 35.59375 54.6875 \nL 46.1875 54.6875 \nL 57.421875 12.015625 \nL 68.609375 54.6875 \nL 77.59375 54.6875 \nL 63.28125 0 \nL 52.6875 0 \nL 40.921875 44.828125 \nL 29.109375 0 \nL 18.5 0 \nz\n\" id=\"DejaVuSans-119\"/>\n       <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n       <path d=\"M 45.40625 46.390625 \nL 45.40625 75.984375 \nL 54.390625 75.984375 \nL 54.390625 0 \nL 45.40625 0 \nL 45.40625 8.203125 \nQ 42.578125 3.328125 38.25 0.953125 \nQ 33.9375 -1.421875 27.875 -1.421875 \nQ 17.96875 -1.421875 11.734375 6.484375 \nQ 5.515625 14.40625 5.515625 27.296875 \nQ 5.515625 40.1875 11.734375 48.09375 \nQ 17.96875 56 27.875 56 \nQ 33.9375 56 38.25 53.625 \nQ 42.578125 51.265625 45.40625 46.390625 \nz\nM 14.796875 27.296875 \nQ 14.796875 17.390625 18.875 11.75 \nQ 22.953125 6.109375 30.078125 6.109375 \nQ 37.203125 6.109375 41.296875 11.75 \nQ 45.40625 17.390625 45.40625 27.296875 \nQ 45.40625 37.203125 41.296875 42.84375 \nQ 37.203125 48.484375 30.078125 48.484375 \nQ 22.953125 48.484375 18.875 42.84375 \nQ 14.796875 37.203125 14.796875 27.296875 \nz\n\" id=\"DejaVuSans-100\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-77\"/>\n      <use x=\"86.279297\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"147.802734\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"209.082031\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"272.460938\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"304.248047\" xlink:href=\"#DejaVuSans-82\"/>\n      <use x=\"369.230469\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"430.753906\" xlink:href=\"#DejaVuSans-119\"/>\n      <use x=\"512.541016\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"573.820312\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"613.183594\" xlink:href=\"#DejaVuSans-100\"/>\n      <use x=\"676.660156\" xlink:href=\"#DejaVuSans-115\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_33\">\n    <path clip-path=\"url(#pf7b7d9e3a1)\" d=\"M 695.669886 32.201761 \nL 619.578977 146.979474 \nL 543.488068 146.979474 \nL 467.397159 146.979474 \nL 391.30625 146.979474 \nL 315.215341 229.874489 \nL 239.124432 229.874489 \nL 163.033523 229.874489 \nL 86.942614 146.979474 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 56.50625 239.758125 \nL 56.50625 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 726.10625 239.758125 \nL 726.10625 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 56.50625 239.758125 \nL 726.10625 239.758125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 56.50625 22.318125 \nL 726.10625 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_19\">\n    <!-- Police iteration performance vs epsilon -->\n    <g transform=\"translate(274.397188 16.318125)scale(0.12 -0.12)\">\n     <defs>\n      <path d=\"M 19.671875 64.796875 \nL 19.671875 37.40625 \nL 32.078125 37.40625 \nQ 38.96875 37.40625 42.71875 40.96875 \nQ 46.484375 44.53125 46.484375 51.125 \nQ 46.484375 57.671875 42.71875 61.234375 \nQ 38.96875 64.796875 32.078125 64.796875 \nz\nM 9.8125 72.90625 \nL 32.078125 72.90625 \nQ 44.34375 72.90625 50.609375 67.359375 \nQ 56.890625 61.8125 56.890625 51.125 \nQ 56.890625 40.328125 50.609375 34.8125 \nQ 44.34375 29.296875 32.078125 29.296875 \nL 19.671875 29.296875 \nL 19.671875 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-80\"/>\n      <path d=\"M 48.78125 52.59375 \nL 48.78125 44.1875 \nQ 44.96875 46.296875 41.140625 47.34375 \nQ 37.3125 48.390625 33.40625 48.390625 \nQ 24.65625 48.390625 19.8125 42.84375 \nQ 14.984375 37.3125 14.984375 27.296875 \nQ 14.984375 17.28125 19.8125 11.734375 \nQ 24.65625 6.203125 33.40625 6.203125 \nQ 37.3125 6.203125 41.140625 7.25 \nQ 44.96875 8.296875 48.78125 10.40625 \nL 48.78125 2.09375 \nQ 45.015625 0.34375 40.984375 -0.53125 \nQ 36.96875 -1.421875 32.421875 -1.421875 \nQ 20.0625 -1.421875 12.78125 6.34375 \nQ 5.515625 14.109375 5.515625 27.296875 \nQ 5.515625 40.671875 12.859375 48.328125 \nQ 20.21875 56 33.015625 56 \nQ 37.15625 56 41.109375 55.140625 \nQ 45.0625 54.296875 48.78125 52.59375 \nz\n\" id=\"DejaVuSans-99\"/>\n      <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n      <path d=\"M 37.109375 75.984375 \nL 37.109375 68.5 \nL 28.515625 68.5 \nQ 23.6875 68.5 21.796875 66.546875 \nQ 19.921875 64.59375 19.921875 59.515625 \nL 19.921875 54.6875 \nL 34.71875 54.6875 \nL 34.71875 47.703125 \nL 19.921875 47.703125 \nL 19.921875 0 \nL 10.890625 0 \nL 10.890625 47.703125 \nL 2.296875 47.703125 \nL 2.296875 54.6875 \nL 10.890625 54.6875 \nL 10.890625 58.5 \nQ 10.890625 67.625 15.140625 71.796875 \nQ 19.390625 75.984375 28.609375 75.984375 \nz\n\" id=\"DejaVuSans-102\"/>\n      <path d=\"M 52 44.1875 \nQ 55.375 50.25 60.0625 53.125 \nQ 64.75 56 71.09375 56 \nQ 79.640625 56 84.28125 50.015625 \nQ 88.921875 44.046875 88.921875 33.015625 \nL 88.921875 0 \nL 79.890625 0 \nL 79.890625 32.71875 \nQ 79.890625 40.578125 77.09375 44.375 \nQ 74.3125 48.1875 68.609375 48.1875 \nQ 61.625 48.1875 57.5625 43.546875 \nQ 53.515625 38.921875 53.515625 30.90625 \nL 53.515625 0 \nL 44.484375 0 \nL 44.484375 32.71875 \nQ 44.484375 40.625 41.703125 44.40625 \nQ 38.921875 48.1875 33.109375 48.1875 \nQ 26.21875 48.1875 22.15625 43.53125 \nQ 18.109375 38.875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.1875 51.21875 25.484375 53.609375 \nQ 29.78125 56 35.6875 56 \nQ 41.65625 56 45.828125 52.96875 \nQ 50 49.953125 52 44.1875 \nz\n\" id=\"DejaVuSans-109\"/>\n      <path d=\"M 2.984375 54.6875 \nL 12.5 54.6875 \nL 29.59375 8.796875 \nL 46.6875 54.6875 \nL 56.203125 54.6875 \nL 35.6875 0 \nL 23.484375 0 \nz\n\" id=\"DejaVuSans-118\"/>\n     </defs>\n     <use xlink:href=\"#DejaVuSans-80\"/>\n     <use x=\"56.677734\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"117.859375\" xlink:href=\"#DejaVuSans-108\"/>\n     <use x=\"145.642578\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"173.425781\" xlink:href=\"#DejaVuSans-99\"/>\n     <use x=\"228.40625\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"289.929688\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"321.716797\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"349.5\" xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"388.708984\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"450.232422\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"491.345703\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"552.625\" xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"591.833984\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"619.617188\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"680.798828\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"744.177734\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"775.964844\" xlink:href=\"#DejaVuSans-112\"/>\n     <use x=\"839.441406\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"900.964844\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"942.078125\" xlink:href=\"#DejaVuSans-102\"/>\n     <use x=\"977.283203\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"1038.464844\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"1077.828125\" xlink:href=\"#DejaVuSans-109\"/>\n     <use x=\"1175.240234\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"1236.519531\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"1299.898438\" xlink:href=\"#DejaVuSans-99\"/>\n     <use x=\"1354.878906\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"1416.402344\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"1448.189453\" xlink:href=\"#DejaVuSans-118\"/>\n     <use x=\"1507.369141\" xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"1559.46875\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"1591.255859\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"1652.779297\" xlink:href=\"#DejaVuSans-112\"/>\n     <use x=\"1716.255859\" xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"1768.355469\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"1796.138672\" xlink:href=\"#DejaVuSans-108\"/>\n     <use x=\"1823.921875\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"1885.103516\" xlink:href=\"#DejaVuSans-110\"/>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pf7b7d9e3a1\">\n   <rect height=\"217.44\" width=\"669.6\" x=\"56.50625\" y=\"22.318125\"/>\n  </clipPath>\n </defs>\n</svg>\n","image/png":"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\n"},"metadata":{"needs_background":"light"}}],"source":["\n","fig, axes = plt.subplots(1,1, figsize = (12,4))\n","# x_axis = np.arange(0,1.1,0.1)\n","x_axis = np.array(eps)\n","x_axis = np.log10(x_axis)\n","print(eps)\n","axes.plot(x_axis, rewards_pi)\n","axes.grid(True)\n","axes.set_xlabel(\"Epsilon\")\n","axes.set_xticks(x_axis)\n","axes.set_xticklabels(eps)\n","axes.set_ylabel(\"Mean Rewards\")\n","axes.set_title('Police iteration performance vs epsilon')\n","\n","# fig.tight_layout()\n",""]},{"cell_type":"markdown","metadata":{},"source":[" ------\n","\n"," ## Conclusion and Discussion\n","\n"," In this assignment, we learn how to use Gym package, how to use Object Oriented Programming idea to build a basic tabular RL algorithm.\n","\n"," It's OK to leave the following cells empty. In the next markdown cell, you can write whatever you like. Like the suggestion on the course, the confusing problems in the assignments, and so on.\n","\n"," If you want to do more investigation, feel free to open new cells via `Esc + B` after the next cells and write codes in it, so that you can reuse some result in this notebook. Remember to write sufficient comments and documents to let others know what you are doing.\n","\n"," Following the submission instruction in the assignment to submit your assignment to our staff. Thank you!\n","\n"," ------"]},{"cell_type":"markdown","metadata":{},"source":[" ..."]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9-final"},"orig_nbformat":2,"kernelspec":{"name":"python37964bitierg5350conda913c8d9a1cb04554aff33b042b05ac09","display_name":"Python 3.7.9 64-bit ('ierg5350': conda)"}}}